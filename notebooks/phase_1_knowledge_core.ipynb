{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118f20dd",
   "metadata": {},
   "source": [
    "<table style=\"border:none; border-collapse:collapse; cellspacing:0; cellpadding:0\">\n",
    "<tr>\n",
    "    <td width=30% style=\"border:none\">\n",
    "        <center>\n",
    "            <img src=\"../images/iapau_icon.png\" width=\"30%\"/><br>\n",
    "            <a href=\"https://iapau.org/\">Association IA Pau</a><br>\n",
    "            <a href=\"https://iapau.org/events/festival/\">Festival IAPau 7</a>\n",
    "        </center>\n",
    "    </td>\n",
    "    <td style=\"border:none\">\n",
    "        <center>\n",
    "            <h1>Atelier - Agentic RAG</h1>\n",
    "            <h2>The Knowledge Core</h2>\n",
    "            <h2>Ingestion, Enrichissemment, et Multi-Modal Indexing</h2>\n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=20% style=\"border:none\">\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "**Pr√©requis :** Compl√©ter d'abord la Phase 0 (acquisition des donn√©es).\n",
    "\n",
    "<img src=\"../images/agentic-rag-data-ingestion-iapau.png\" alt=\"data-ingestion\" width=\"70%\"/>\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table des mati√®res\n",
    "\n",
    "- [**Import des biblioth√®ques et chargement des donn√©es**](#import-donnees)\n",
    "\n",
    "1. [**Segmentation avanc√©e des documents**](#analyse-documents)\n",
    "   - Extraction du contenu des documents\n",
    "   - Segmentation avec la biblioth√®que `unstructured`\n",
    "\n",
    "2. [**D√©coupage s√©mantique/structur√©**](#decoupage-intelligent)\n",
    "   - Chunking conscient de la structure\n",
    "   - Pr√©servation des informations hi√©rarchiques\n",
    "\n",
    "3. [**Enrichissement avec LLM**](#enrichissement-llm)\n",
    "   - G√©n√©ration de m√©tadonn√©es structur√©es\n",
    "   - Enrichissement des chunks avec r√©sum√©s et mots-cl√©s\n",
    "\n",
    "4. [**Emdeddings & Vector Database (Qdrant)**](#magasin-vectoriel)\n",
    "   - G√©n√©ration d'embeddings\n",
    "   - Population de la base vectorielle Qdrant\n",
    "\n",
    "5. [**Cr√©ation de la base de donn√©es SQL**](#base-donnees-sql)\n",
    "   - Cr√©ation de la base SQLite\n",
    "   - Structuration des donn√©es financi√®res\n",
    "\n",
    "- [**Prochaines √©tapes**](#phase-terminee)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5155d4c",
   "metadata": {},
   "source": [
    "<a id=\"import-donnees\"></a>\n",
    "Import des biblioth√®ques et chargement des donn√©es (Phase 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c3ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from fastembed import TextEmbedding\n",
    "import qdrant_client\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d29206a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 files from Phase 0\n",
      "\n",
      "Loaded structured data with 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Load data from Phase 0\n",
    "COMPANY_TICKER = \"NVDA\"\n",
    "DATA_PATH = Path(f\"sec-edgar-filings/{COMPANY_TICKER}/\")\n",
    "CSV_PATH = \"revenue_summary.csv\"\n",
    "\n",
    "# Find all SEC submission files\n",
    "all_files = list(DATA_PATH.rglob(\"full-submission.txt\"))\n",
    "print(f\"Loaded {len(all_files)} files from Phase 0\")\n",
    "\n",
    "# Load the CSV data\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"\\nLoaded structured data with {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a9e30d",
   "metadata": {},
   "source": [
    "<a id=\"analyse-documents\"></a>\n",
    "### <b><div style='padding:15px;background-color:#4A5568;color:white;border-radius:2px;font-size:110%;text-align: left'>1. Segmentation avanc√©e des documents</div></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e911670f",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0f7ff,#ffffff);padding:16px;border-left:6px solid #2b6cb0;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>üöÄ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Ce que nous allons faire</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous allons utiliser la biblioth√®que <a href=\"https://docs.unstructured.io\">unstructured</a>  pour analyser les d√©p√¥ts HTML bruts. Contrairement √† une simple extraction de texte, <a href=\"https://docs.unstructured.io\">unstructured</a>  partitionne le document en une liste d'¬´ √©l√©ments ¬ª significatifs tels que <code>Title</code>, <code>NarrativeText</code>, <code>ListItem</code> et <code>Table</code>. Cette pr√©servation de l'information structurelle est la premi√®re et la plus critique √©tape vers un d√©coupage s√©mantique.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5869af54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: sec-edgar-filings/NVDA/10-K/0001045810-25-000023/full-submission.txt...\n",
      "Extracted HTML content: 2,067,275 characters\n",
      "\n",
      "Successfully parsed into 947 elements.\n",
      "\n",
      "--- Distribution des types d'√©l√©ments ---\n",
      "UncategorizedText: 426\n",
      "NarrativeText: 381\n",
      "ListItem: 109\n",
      "Table: 29\n",
      "Image: 2\n",
      "\n",
      "================================================================================\n",
      "EXEMPLES DE DIFF√âRENTS TYPES D'√âL√âMENTS\n",
      "================================================================================\n",
      "\n",
      "--- Exemple de TITRE (Title) ---\n",
      "Aucun √©l√©ment Title trouv√© - tous les √©l√©ments sont probablement class√©s comme UncategorizedText\n",
      "\n",
      "--- Exemple de TEXTE NARRATIF (NarrativeText) ---\n",
      "Contenu: Indicate by check mark if the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act. Yes ‚òê ‚òí...\n",
      "\n",
      "--- Exemple de TABLEAU (Table) ---\n",
      "Repr√©sentation HTML:\n",
      "<table><tr><td/><td/><td/><td/><td/><td/></tr><tr><td>‚òí</td><td>ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934</td></tr></table>...\n",
      "\n",
      "================================================================================\n",
      "NOTE: Si vous ne voyez que 'UncategorizedText', c'est normal pour les fichiers SEC.\n",
      "Le HTML des d√©p√¥ts SEC est tr√®s complexe et unstructured a du mal √† d√©tecter\n",
      "la structure automatiquement. Les chunks fonctionneront quand m√™me correctement.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def extract_html_from_sec_file(file_path) -> str:\n",
    "    \"\"\"Extracts the HTML content from an SEC submission file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    match = re.search(r'<html[^>]*>.*?</html>', content, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return \"\"\n",
    "\n",
    "def parse_html_file(file_path):\n",
    "    \"\"\"Parses an HTML file using unstructured and returns a list of elements.\"\"\"\n",
    "    try:\n",
    "        html_content = extract_html_from_sec_file(file_path)\n",
    "        \n",
    "        if not html_content:\n",
    "            print(\"No HTML content found in file\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Extracted HTML content: {len(html_content):,} characters\")\n",
    "        \n",
    "        from io import BytesIO\n",
    "        # Try with more aggressive detection settings\n",
    "        elements = partition_html(\n",
    "            file=BytesIO(html_content.encode('utf-8')),\n",
    "            # Use hi_res for better element detection (slower but more accurate)\n",
    "            # Note: This requires detectron2 for table detection\n",
    "            # For now, we use default but with include_metadata\n",
    "            include_metadata=True,\n",
    "        )\n",
    "        return elements\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {file_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# Test parsing on first 10-K file\n",
    "ten_k_file = next(f for f in all_files if \"10-K\" in str(f))\n",
    "print(f\"Parsing file: {ten_k_file}...\")\n",
    "\n",
    "parsed_elements = parse_html_file(ten_k_file)\n",
    "\n",
    "print(f\"\\nSuccessfully parsed into {len(parsed_elements)} elements.\")\n",
    "\n",
    "# Show element type distribution\n",
    "from collections import Counter\n",
    "element_types = Counter(elem.category if hasattr(elem, 'category') else type(elem).__name__ \n",
    "                        for elem in parsed_elements)\n",
    "print(\"\\n--- Distribution des types d'√©l√©ments ---\")\n",
    "for elem_type, count in element_types.most_common():\n",
    "    print(f\"{elem_type}: {count}\")\n",
    "\n",
    "# Find and display specific element types\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXEMPLES DE DIFF√âRENTS TYPES D'√âL√âMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find examples of each type\n",
    "title_example = None\n",
    "narrative_example = None\n",
    "table_example = None\n",
    "\n",
    "for element in parsed_elements:\n",
    "    elem_type = element.category if hasattr(element, 'category') else type(element).__name__\n",
    "    \n",
    "    if elem_type == \"Title\" and title_example is None:\n",
    "        title_example = element\n",
    "    elif elem_type == \"NarrativeText\" and narrative_example is None and len(str(element)) > 100:\n",
    "        narrative_example = element\n",
    "    elif elem_type == \"Table\" and table_example is None:\n",
    "        table_example = element\n",
    "    \n",
    "    if title_example and narrative_example and table_example:\n",
    "        break\n",
    "\n",
    "# Display Title example\n",
    "if title_example:\n",
    "    print(\"\\n--- Exemple de TITRE (Title) ---\")\n",
    "    print(f\"Contenu: {str(title_example)}\")\n",
    "else:\n",
    "    print(\"\\n--- Exemple de TITRE (Title) ---\")\n",
    "    print(\"Aucun √©l√©ment Title trouv√© - tous les √©l√©ments sont probablement class√©s comme UncategorizedText\")\n",
    "\n",
    "# Display NarrativeText example\n",
    "if narrative_example:\n",
    "    print(\"\\n--- Exemple de TEXTE NARRATIF (NarrativeText) ---\")\n",
    "    print(f\"Contenu: {str(narrative_example)[:500]}...\")\n",
    "else:\n",
    "    print(\"\\n--- Exemple de TEXTE NARRATIF (NarrativeText) ---\")\n",
    "    print(\"Aucun √©l√©ment NarrativeText trouv√©\")\n",
    "\n",
    "# Display Table example\n",
    "if table_example:\n",
    "    print(\"\\n--- Exemple de TABLEAU (Table) ---\")\n",
    "    # Check if table has HTML metadata\n",
    "    if hasattr(table_example, 'metadata'):\n",
    "        table_metadata = table_example.metadata.to_dict() if hasattr(table_example.metadata, 'to_dict') else {}\n",
    "        if 'text_as_html' in table_metadata:\n",
    "            print(f\"Repr√©sentation HTML:\\n{table_metadata['text_as_html'][:800]}...\")\n",
    "        else:\n",
    "            print(f\"Repr√©sentation texte:\\n{str(table_example)[:500]}...\")\n",
    "    else:\n",
    "        print(f\"Repr√©sentation texte:\\n{str(table_example)[:500]}...\")\n",
    "else:\n",
    "    print(\"\\n--- Exemple de TABLEAU (Table) ---\")\n",
    "    print(\"Aucun √©l√©ment Table trouv√©\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOTE: Si vous ne voyez que 'UncategorizedText', c'est normal pour les fichiers SEC.\")\n",
    "print(\"Le HTML des d√©p√¥ts SEC est tr√®s complexe et unstructured a du mal √† d√©tecter\")\n",
    "print(\"la structure automatiquement. Les chunks fonctionneront quand m√™me correctement.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a27e2",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>La sortie montre que nous avons r√©ussi √† partitionner le document 10-K en plusieurs centaines d'√©l√©ments individuels. L'exemple de sortie est crucial : il d√©montre que <a href=\"https://docs.unstructured.io\">unstructured</a>  a identifi√© diff√©rents types de contenu. Nous pouvons voir <code>Title</code> et <code>NarrativeText</code>. Cette prise en compte de la structure est ce que nous allons exploiter dans l'√©tape suivante pour cr√©er des fragments s√©mantiques, en particulier pour pr√©server les tableaux.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6fe524",
   "metadata": {},
   "source": [
    "<a id=\"decoupage-intelligent\"></a>\n",
    "### <b><div style='padding:15px;background-color:#4A5568;color:white;border-radius:2px;font-size:110%;text-align: left'>2. D√©coupage s√©mantique</div></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de2840",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0f7ff,#ffffff);padding:16px;border-left:6px solid #2b6cb0;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>üöÄ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Ce que nous allons faire</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Les m√©thodes de d√©coupage standard (comme la division par un nombre fixe de tokens) peuvent √™tre destructrices, en particulier pour les documents financiers o√π les tableaux sont critiques. Un tableau coup√© en deux perd tout son sens. Nous allons utiliser la strat√©gie <code>chunk_by_title</code>. Cette m√©thode regroupe le texte sous les titres et, surtout, tente de conserver les tableaux entiers, en les traitant comme des unit√©s atomiques.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "550702cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document chunked into 168 sections.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    ---\n",
       "    ### üìÑ Exemple de Chunk de Texte\n",
       "\n",
       "    **Type:** Texte narratif  \n",
       "    **Longueur:** 871 caract√®res\n",
       "\n",
       "    **Contenu:**\n",
       "    ```\n",
       "    The aggregate market value of the voting stock held by non-affiliates of the registrant as of July 26, 2024 was approximately $ trillion (based on the closing sales price of the registrant's common stock as reported by the Nasdaq Global Select Market on July 26, 2024). This calculation excludes 1.0 billion shares held by directors and executive officers of the registrant. This calculation does not exclude shares held by such organizations whose ownership exceeds 5% of the registrant's outstanding common stock that have represented to the registrant that they are registered investment advisers or investment companies registered under section 8 of the Investment Company Act of 1940.\n",
       "\n",
       "The numbe...\n",
       "    ```\n",
       "    ---\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    ---\n",
       "    ### üìä Exemple de Chunk de Tableau\n",
       "\n",
       "    **Type:** Tableau (pr√©serv√© avec HTML)  \n",
       "    **M√©tadonn√©e:** `text_as_html` pr√©sent = ‚úÖ\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rendu du tableau:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td/><td/><td/><td/><td/><td/></tr><tr><td>‚òí</td><td>ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934</td></tr></table> <table><tr><td/><td/><td/><td/><td/><td/></tr><tr><td>‚òê</td><td>TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934</td></tr></table> <table><tr><td/><td/><td/><td/><td/><td/></tr><tr><td>Delaware</td><td>94-3177549</td></tr><tr><td>(State or other jurisdiction of</td><td>(I.R.S. Employer</td></tr><tr><td>incorporation or organization)</td><td>Identification No.)</td></tr><tr><td/><td/></tr><tr><td>2788 San Tomas Expressway , Santa Clara , California</td><td>95051</td></tr><tr><td>(Address of principal executive offices)</td><td>(Zip Code)</td></tr></table> <table><tr><td/><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td>Title of each class</td><td>Trading Symbol(s)</td><td>Name of each exchange on which registered</td></tr><tr><td>Common Stock, $0.001 par value per share</td><td>NVDA</td><td>The Nasdaq Global Select Market</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    **Code HTML source (extrait):**\n",
       "    ```html\n",
       "    <table><tr><td/><td/><td/><td/><td/><td/></tr><tr><td>‚òí</td><td>ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934</td></tr></table> <table><tr><td/><td/><td/><td/><td/><td/></tr><tr><td>‚òê</td><td>TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934</td></tr></table> <table><tr><td/><td/><td/><td/><td/><td/></tr><tr><td>Delaware</td><td>94-3177549</td></tr><tr><td>(State or other jurisdiction of</td><td>(I.R.S. Employer</td></tr><tr><td>incorporation or organization)</td><td>Identification No.)</td></tr><tr><td/><td/></tr><t...\n",
       "    ```\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunks = chunk_by_title(\n",
    "    parsed_elements,\n",
    "    max_characters=2048,\n",
    "    combine_text_under_n_chars=256,\n",
    "    new_after_n_chars=1800\n",
    ")\n",
    "\n",
    "print(f\"Document chunked into {len(chunks)} sections.\")\n",
    "\n",
    "# Find sample chunks\n",
    "text_chunk_sample = None\n",
    "table_chunk_sample = None\n",
    "\n",
    "for chunk in chunks:\n",
    "    chunk_metadata = chunk.metadata.to_dict() if hasattr(chunk.metadata, 'to_dict') else {}\n",
    "    if 'text_as_html' not in chunk_metadata and text_chunk_sample is None and len(str(chunk)) > 500:\n",
    "        text_chunk_sample = chunk\n",
    "    if 'text_as_html' in chunk_metadata and table_chunk_sample is None:\n",
    "        table_chunk_sample = chunk\n",
    "    if text_chunk_sample and table_chunk_sample:\n",
    "        break\n",
    "\n",
    "# Display with Markdown formatting\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "if text_chunk_sample:\n",
    "    display(Markdown(f\"\"\"\n",
    "    ---\n",
    "    ### üìÑ Exemple de Chunk de Texte\n",
    "\n",
    "    **Type:** Texte narratif  \n",
    "    **Longueur:** {len(str(text_chunk_sample))} caract√®res\n",
    "\n",
    "    **Contenu:**\n",
    "    ```\n",
    "    {str(text_chunk_sample)[:700]}...\n",
    "    ```\n",
    "    ---\n",
    "    \"\"\"))\n",
    "\n",
    "if table_chunk_sample:\n",
    "    table_metadata = table_chunk_sample.metadata.to_dict() if hasattr(table_chunk_sample.metadata, 'to_dict') else {}\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ---\n",
    "    ### üìä Exemple de Chunk de Tableau\n",
    "\n",
    "    **Type:** Tableau (pr√©serv√© avec HTML)  \n",
    "    **M√©tadonn√©e:** `text_as_html` pr√©sent = ‚úÖ\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Display the actual HTML table if available\n",
    "    if 'text_as_html' in table_metadata:\n",
    "        display(Markdown(\"**Rendu du tableau:**\"))\n",
    "        display(HTML(table_metadata['text_as_html']))\n",
    "        \n",
    "        display(Markdown(f\"\"\"\n",
    "    **Code HTML source (extrait):**\n",
    "    ```html\n",
    "    {table_metadata['text_as_html'][:600]}...\n",
    "    ```\n",
    "    \"\"\"))\n",
    "    else:\n",
    "        display(Markdown(f\"\"\"\n",
    "    **Contenu texte:**\n",
    "    ```\n",
    "    {str(table_chunk_sample)[:500]}...\n",
    "    ```\n",
    "    \"\"\"))\n",
    "    \n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f89ad32",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>La sortie montre que nous avons r√©duit des centaines d'√©l√©ments en quelques fragments. Le point cl√© se trouve dans les exemples de fragments. Nous voyons un fragment de texte standard, et plus important encore, un fragment de tableau. Notez que les m√©tadonn√©es du fragment de tableau incluent <code>text_as_html</code>. Cela indique que <a href=\"https://docs.unstructured.io\">unstructured</a> a correctement identifi√© et pr√©serv√© un tableau, ce qui est un √©norme avantage pour la qualit√© des donn√©es. Nous avons r√©ussi √† √©viter de d√©truire des donn√©es tabulaires critiques pendant le processus de d√©coupage.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8410a",
   "metadata": {},
   "source": [
    "<a id=\"enrichissement-llm\"></a>\n",
    "### <b><div style='padding:15px;background-color:#4A5568;color:white;border-radius:2px;font-size:110%;text-align: left'>3. Enrichissement avec LLM</div></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651475b6",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0f7ff,#ffffff);padding:16px;border-left:6px solid #2b6cb0;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>üöÄ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Ce que nous allons faire</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Il s'agit d'une pierre angulaire de notre pipeline RAG avanc√©. Au lieu d'int√©grer simplement du texte brut, nous utiliserons un LLM rapide et puissant pour g√©n√©rer des m√©tadonn√©es pour chaque fragment. Ces m√©tadonn√©es agissent comme des ¬´ signaux ¬ª suppl√©mentaires pour notre syst√®me, lui permettant de comprendre le contenu √† un niveau beaucoup plus profond.</p>\n",
    "    <p style='margin:0 8px 0 0;font-weight:600;color:#000;'>Pour chaque fragment, nous g√©n√©rerons :</p>\n",
    "    <ul style='margin:8px 0 0 18px;'>\n",
    "      <li><strong>R√©sum√©</strong> : Un r√©sum√© concis de 1 √† 2 phrases.</li>\n",
    "      <li><strong>Mots-cl√©s</strong> : Une liste de sujets cl√©s.</li>\n",
    "      <li><strong>Questions hypoth√©tiques</strong> : Une liste de questions auxquelles le fragment peut r√©pondre.</li>\n",
    "      <li><strong>R√©sum√© de tableau (pour les tableaux uniquement)</strong> : Une description en langage naturel des principales informations du tableau.</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8fd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic model for metadata defined.\n",
      "{\n",
      "  \"description\": \"Structured metadata for a document chunk.\",\n",
      "  \"properties\": {\n",
      "    \"summary\": {\n",
      "      \"description\": \"A concise 1-2 sentence summary of the chunk.\",\n",
      "      \"title\": \"Summary\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"keywords\": {\n",
      "      \"description\": \"A list of 5-7 key topics or entities mentioned.\",\n",
      "      \"items\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"title\": \"Keywords\",\n",
      "      \"type\": \"array\"\n",
      "    },\n",
      "    \"hypothetical_questions\": {\n",
      "      \"description\": \"A list of 3-5 questions this chunk could answer.\",\n",
      "      \"items\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"title\": \"Hypothetical Questions\",\n",
      "      \"type\": \"array\"\n",
      "    },\n",
      "    \"table_summary\": {\n",
      "      \"anyOf\": [\n",
      "        {\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "          \"type\": \"null\"\n",
      "        }\n",
      "      ],\n",
      "      \"default\": null,\n",
      "      \"description\": \"If the chunk is a table, a natural language summary of its key insights.\",\n",
      "      \"title\": \"Table Summary\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"summary\",\n",
      "    \"keywords\",\n",
      "    \"hypothetical_questions\"\n",
      "  ],\n",
      "  \"title\": \"ChunkMetadata\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Pydantic permet de d√©finir la structure JSON souhait√©e, garantissant que la sortie du LLM est fiable.\n",
    "\n",
    "class ChunkMetadata(BaseModel):\n",
    "    \"\"\"Structured metadata for a document chunk.\"\"\"\n",
    "    summary: str = Field(description=\"A concise 1-2 sentence summary of the chunk.\")\n",
    "    keywords: List[str] = Field(description=\"A list of 5-7 key topics or entities mentioned.\")\n",
    "    hypothetical_questions: List[str] = Field(description=\"A list of 3-5 questions this chunk could answer.\")\n",
    "    table_summary: Optional[str] = Field(description=\"If the chunk is a table, a natural language summary of its key insights.\", default=None)\n",
    "\n",
    "print(\"Pydantic model for metadata defined.\")\n",
    "print(json.dumps(ChunkMetadata.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a28938",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous avons d√©fini le mod√®le Pydantic <code>ChunkMetadata</code>. L'affichage du sch√©ma JSON montre la structure exacte, y compris les noms de champs, les types et les descriptions, que nous demanderons au LLM. Cette utilisation de sorties structur√©es est bien plus fiable que la simple ing√©nierie de prompt.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa22c6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enrichment functions and LLM are ready.\n"
     ]
    }
   ],
   "source": [
    "enrichment_llm = ChatOpenAI(model='gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=0.).with_structured_output(ChunkMetadata)\n",
    "\n",
    "def generate_enrichment_prompt(chunk_text: str, is_table: bool) -> str:\n",
    "    table_instruction = \"\"\"\n",
    "    This chunk is a TABLE. Your summary should describe the main data points and trends.\n",
    "    \"\"\" if is_table else \"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert financial analyst. Please analyze the following document chunk and generate the specified metadata.\n",
    "    {table_instruction}\n",
    "    Chunk Content:\n",
    "    ---\n",
    "    {chunk_text}\n",
    "    ---\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def enrich_chunk(chunk) -> Dict[str, Any]:\n",
    "    chunk_metadata = chunk.metadata.to_dict() if hasattr(chunk.metadata, 'to_dict') else {}\n",
    "    is_table = 'text_as_html' in chunk_metadata\n",
    "    content = chunk_metadata.get('text_as_html') if is_table else str(chunk)\n",
    "    \n",
    "    truncated_content = content[:3000]\n",
    "    prompt = generate_enrichment_prompt(truncated_content, is_table)\n",
    "    \n",
    "    try:\n",
    "        metadata_obj = enrichment_llm.invoke(prompt)\n",
    "        return metadata_obj.model_dump()\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error enriching chunk: {type(e).__name__}: {str(e)[:200]}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"Enrichment functions and LLM are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2ecd0",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous avons mis en place la logique de base pour l'enrichissement. Nous avons instanci√© un LLM (<i>gpt-4o-mini</i>) et l'avons li√© √† notre mod√®le Pydantic. La fonction <code>enrich_chunk</code> identifie correctement si un fragment est un tableau, le tronque √† une taille g√©rable et appelle le LLM pour g√©n√©rer les m√©tadonn√©es structur√©es. Maintenant, testons-le.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e20e7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "---\n",
       "### üîç Enrichissement d'un Chunk de Texte\n",
       "\n",
       "**R√©sultat de l'enrichissement LLM:**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**üìù R√©sum√©:**\n",
       "> The document chunk provides information on the market value of voting stock held by non-affiliates of a registrant as of July 26, 2024, and mentions the number of shares outstanding as of February 21, 2025.\n",
       "\n",
       "**üè∑Ô∏è Mots-cl√©s:**\n",
       "`market value`, `voting stock`, `non-affiliates`, `common stock`, `NVIDIA Corporation`, `Investment Company Act`, `shares outstanding`\n",
       "\n",
       "**‚ùì Questions hypoth√©tiques:**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "1. *What was the market value of the voting stock as of July 26, 2024?*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2. *How many shares of common stock were outstanding as of February 21, 2025?*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "3. *What exclusions were made in the calculation of the market value?*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4. *Who are considered non-affiliates in this context?*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "5. *What is the significance of the Investment Company Act of 1940 in this document?*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**üìä JSON complet:**\n",
       "```json\n",
       "{\n",
       "  \"summary\": \"The document chunk provides information on the market value of voting stock held by non-affiliates of a registrant as of July 26, 2024, and mentions the number of shares outstanding as of February 21, 2025.\",\n",
       "  \"keywords\": [\n",
       "    \"market value\",\n",
       "    \"voting stock\",\n",
       "    \"non-affiliates\",\n",
       "    \"common stock\",\n",
       "    \"NVIDIA Corporation\",\n",
       "    \"Investment Company Act\",\n",
       "    \"shares outstanding\"\n",
       "  ],\n",
       "  \"hypothetical_questions\": [\n",
       "    \"What was the market value of the voting stock as of July 26, 2024?\",\n",
       "    \"How many shares of common stock were outstanding as of February 21, 2025?\",\n",
       "    \"What exclusions were made in the calculation of the market value?\",\n",
       "    \"Who are considered non-affiliates in this context?\",\n",
       "    \"What is the significance of the Investment Company Act of 1940 in this document?\"\n",
       "  ],\n",
       "  \"table_summary\": null\n",
       "}\n",
       "```\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### üìä Enrichissement d'un Chunk de Tableau\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**üìù R√©sum√©:**\n",
       "> The document chunk contains key information from a financial report, including the type of report (annual), the company's state of incorporation (Delaware), its IRS identification number, address, and details about its common stock listing on the Nasdaq.\n",
       "\n",
       "**üè∑Ô∏è Mots-cl√©s:**\n",
       "`Annual Report`, `Securities Exchange Act`, `Delaware`, `IRS Identification Number`, `Common Stock`, `Trading Symbol`, `Nasdaq`\n",
       "\n",
       "**‚ùì Questions hypoth√©tiques:**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "1. *What type of report is being filed?*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2. *What is the company's state of incorporation?*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "3. *What is the trading symbol for the company's stock?*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4. *Which exchange is the company's stock listed on?*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "5. *What is the address of the company's principal executive offices?*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**üìã R√©sum√© du tableau (sp√©cifique):**\n",
       "> The tables provide essential details about the company's annual report filing, its incorporation in Delaware, its IRS identification number, and the specifics of its common stock listing on the Nasdaq.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**üìä JSON complet:**\n",
       "```json\n",
       "{\n",
       "  \"summary\": \"The document chunk contains key information from a financial report, including the type of report (annual), the company's state of incorporation (Delaware), its IRS identification number, address, and details about its common stock listing on the Nasdaq.\",\n",
       "  \"keywords\": [\n",
       "    \"Annual Report\",\n",
       "    \"Securities Exchange Act\",\n",
       "    \"Delaware\",\n",
       "    \"IRS Identification Number\",\n",
       "    \"Common Stock\",\n",
       "    \"Trading Symbol\",\n",
       "    \"Nasdaq\"\n",
       "  ],\n",
       "  \"hypothetical_questions\": [\n",
       "    \"What type of report is being filed?\",\n",
       "    \"What is the company's state of incorporation?\",\n",
       "    \"What is the trading symbol for the company's stock?\",\n",
       "    \"Which exchange is the company's stock listed on?\",\n",
       "    \"What is the address of the company's principal executive offices?\"\n",
       "  ],\n",
       "  \"table_summary\": \"The tables provide essential details about the company's annual report filing, its incorporation in Delaware, its IRS identification number, and the specifics of its common stock listing on the Nasdaq.\"\n",
       "}\n",
       "```\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test enrichment on text chunk\n",
    "enriched_text_meta = enrich_chunk(text_chunk_sample)\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "---\n",
    "### üîç Enrichissement d'un Chunk de Texte\n",
    "\n",
    "**R√©sultat de l'enrichissement LLM:**\n",
    "\"\"\"))\n",
    "\n",
    "if enriched_text_meta:\n",
    "    display(Markdown(f\"\"\"\n",
    "**üìù R√©sum√©:**\n",
    "> {enriched_text_meta.get('summary', 'N/A')}\n",
    "\n",
    "**üè∑Ô∏è Mots-cl√©s:**\n",
    "{', '.join([f'`{kw}`' for kw in enriched_text_meta.get('keywords', [])])}\n",
    "\n",
    "**‚ùì Questions hypoth√©tiques:**\n",
    "\"\"\"))\n",
    "    for i, q in enumerate(enriched_text_meta.get('hypothetical_questions', []), 1):\n",
    "        display(Markdown(f\"{i}. *{q}*\"))\n",
    "    \n",
    "    display(Markdown(f\"\"\"\n",
    "**üìä JSON complet:**\n",
    "```json\n",
    "{json.dumps(enriched_text_meta, indent=2, ensure_ascii=False)}\n",
    "```\n",
    "---\n",
    "\"\"\"))\n",
    "\n",
    "# Test enrichment on table chunk\n",
    "display(Markdown(\"\"\"\n",
    "### üìä Enrichissement d'un Chunk de Tableau\n",
    "\"\"\"))\n",
    "\n",
    "enriched_table_meta = enrich_chunk(table_chunk_sample)\n",
    "\n",
    "if enriched_table_meta:\n",
    "    display(Markdown(f\"\"\"\n",
    "**üìù R√©sum√©:**\n",
    "> {enriched_table_meta.get('summary', 'N/A')}\n",
    "\n",
    "**üè∑Ô∏è Mots-cl√©s:**\n",
    "{', '.join([f'`{kw}`' for kw in enriched_table_meta.get('keywords', [])])}\n",
    "\n",
    "**‚ùì Questions hypoth√©tiques:**\n",
    "\"\"\"))\n",
    "    for i, q in enumerate(enriched_table_meta.get('hypothetical_questions', []), 1):\n",
    "        display(Markdown(f\"{i}. *{q}*\"))\n",
    "    \n",
    "    if enriched_table_meta.get('table_summary'):\n",
    "        display(Markdown(f\"\"\"\n",
    "**üìã R√©sum√© du tableau (sp√©cifique):**\n",
    "> {enriched_table_meta.get('table_summary')}\n",
    "\"\"\"))\n",
    "    \n",
    "    display(Markdown(f\"\"\"\n",
    "**üìä JSON complet:**\n",
    "```json\n",
    "{json.dumps(enriched_table_meta, indent=2, ensure_ascii=False)}\n",
    "```\n",
    "---\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea9a22",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>C'est un r√©sultat fantastique. La sortie montre deux objets JSON, un pour chaque type de fragment.</p>\n",
    "    <p style='margin:0 8px 0 0;font-weight:600;color:#000;'></p>\n",
    "    <ul style='margin:8px 0 0 18px;color:#000;'>\n",
    "      <li>Pour le fragment de texte, nous avons un r√©sum√© clair, des mots-cl√©s pertinents et des questions hypoth√©tiques perspicaces.</li><br>\n",
    "      <li>Pour le fragment de tableau, le LLM l'a correctement identifi√© comme un tableau et a fourni un `table_summary` qui interpr√®te les donn√©es en langage naturel. C'est incroyablement puissant. D√©sormais, une recherche s√©mantique pour ¬´ croissance du chiffre d'affaires par segment ¬ª pourrait correspondre √† ce tableau, m√™me si ces mots exacts ne figurent pas dans le HTML brut.</li>\n",
    "    </ul>\n",
    "    <br>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Maintenant, nous allons appliquer ceci √† tous nos documents.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "002d6109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing enriched chunks file. Loading from disk.\n"
     ]
    }
   ],
   "source": [
    "ENRICHED_CHUNKS_PATH = 'enriched_chunks.json'\n",
    "\n",
    "if os.path.exists(ENRICHED_CHUNKS_PATH):\n",
    "    print(\"Found existing enriched chunks file. Loading from disk.\")\n",
    "    with open(ENRICHED_CHUNKS_PATH, 'r') as f:\n",
    "        all_enriched_chunks = json.load(f)\n",
    "else:\n",
    "    all_enriched_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66fc272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files (this takes time - run only if needed)\n",
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "\n",
    "def process_file(file_path: Path) -> tuple[List[Dict[str, Any]], int, int]:\n",
    "    \"\"\"Process a single SEC filing and return enriched chunks with stats.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (enriched_chunks, success_count, error_count)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_elements = parse_html_file(file_path)\n",
    "        if not parsed_elements:\n",
    "            print(f\"‚ö†Ô∏è  No elements parsed from {file_path.name}\")\n",
    "            return [], 0, 0\n",
    "        \n",
    "        doc_chunks = chunk_by_title(\n",
    "            parsed_elements, \n",
    "            max_characters=2048, \n",
    "            combine_text_under_n_chars=256\n",
    "        )\n",
    "        \n",
    "        print(f\"  üìÑ Created {len(doc_chunks)} chunks, starting enrichment...\")\n",
    "        \n",
    "        enriched_chunks = []\n",
    "        success_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        for idx, chunk in enumerate(tqdm(doc_chunks, desc=\"Enriching chunks\", leave=False), 1):\n",
    "            try:\n",
    "                enrichment_data = enrich_chunk(chunk)\n",
    "                \n",
    "                if enrichment_data:\n",
    "                    chunk_metadata = chunk.metadata.to_dict() if hasattr(chunk.metadata, 'to_dict') else {}\n",
    "                    is_table = 'text_as_html' in chunk_metadata\n",
    "                    content = chunk_metadata.get('text_as_html') if is_table else str(chunk)\n",
    "                    \n",
    "                    enriched_chunks.append({\n",
    "                        'source': f\"{file_path.parent.parent.name}/{file_path.parent.name}\",\n",
    "                        'content': content,\n",
    "                        'is_table': is_table,\n",
    "                        **enrichment_data\n",
    "                    })\n",
    "                    success_count += 1\n",
    "                else:\n",
    "                    error_count += 1\n",
    "                    print(f\"    ‚ö†Ô∏è  Chunk {idx}/{len(doc_chunks)} failed enrichment\")\n",
    "                \n",
    "                # Small delay to avoid overwhelming the LLM endpoint\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                print(f\"    ‚ùå Chunk {idx}/{len(doc_chunks)} raised exception: {type(e).__name__}\")\n",
    "        \n",
    "        return enriched_chunks, success_count, error_count\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical error processing {file_path.name}: {type(e).__name__}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [], 0, 0\n",
    "\n",
    "# Process all files\n",
    "display(Markdown(f\"\"\"\n",
    "### üîÑ Traitement de {len(all_files)} fichiers SEC\n",
    "\n",
    "Ce processus va:\n",
    "1. Parser chaque fichier HTML\n",
    "2. D√©couper en chunks s√©mantiques\n",
    "3. Enrichir chaque chunk avec le LLM\n",
    "4. Sauvegarder progressivement dans `{ENRICHED_CHUNKS_PATH}`\n",
    "\n",
    "**‚è±Ô∏è Temps estim√©:** ~{len(all_files) * 20} minutes\n",
    "\"\"\"))\n",
    "\n",
    "total_success = 0\n",
    "total_errors = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i, file_path in enumerate(tqdm(all_files, desc=\"Processing files\"), 1):\n",
    "    file_start = time.time()\n",
    "    \n",
    "    file_chunks, success, errors = process_file(file_path)\n",
    "    all_enriched_chunks.extend(file_chunks)\n",
    "    total_success += success\n",
    "    total_errors += errors\n",
    "    \n",
    "    # Save progress after each file\n",
    "    with open(ENRICHED_CHUNKS_PATH, 'w') as f:\n",
    "        json.dump(all_enriched_chunks, f, indent=2)\n",
    "    \n",
    "    file_time = time.time() - file_start\n",
    "    elapsed = time.time() - start_time\n",
    "    avg_time = elapsed / i\n",
    "    eta = avg_time * (len(all_files) - i)\n",
    "    \n",
    "    display(Markdown(f\"\"\"‚úÖ **Fichier {i}/{len(all_files)}**: `{file_path.name}`  \n",
    "‚Üí {len(file_chunks)} chunks enrichis ({success} ‚úÖ, {errors} ‚ùå)  \n",
    "‚Üí Total cumul√©: {len(all_enriched_chunks)} chunks  \n",
    "‚Üí Temps: {file_time:.1f}s | ETA: {eta/60:.1f} min\"\"\"))\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "---\n",
    "### ‚ú® Traitement termin√©!\n",
    "\n",
    "**Total:** {len(all_enriched_chunks)} chunks enrichis  \n",
    "**Succ√®s:** {total_success} ‚úÖ  \n",
    "**Erreurs:** {total_errors} ‚ùå  \n",
    "**Taux de r√©ussite:** {100*total_success/(total_success+total_errors):.1f}%  \n",
    "**Temps total:** {(time.time()-start_time)/60:.1f} minutes  \n",
    "**Sauvegard√© dans:** `{ENRICHED_CHUNKS_PATH}`\n",
    "---\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03704d11",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Cette cellule prend un certain temps √† s'ex√©cuter car elle implique de nombreux appels LLM. La sortie contient tous les fragments enrichis en m√©tadonn√©es, que nous avons cr√©√©s √† partir de tous les d√©p√¥ts SEC. Fait crucial, nous avons enregistr√© ce r√©sultat dans un fichier JSON. Il s'agit d'une bonne pratique essentielle pour sauvegarder notre progression et √©viter de r√©ex√©cuter des √©tapes co√ªteuses de traitement des donn√©es.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b1ae5",
   "metadata": {},
   "source": [
    "<a id=\"magasin-vectoriel\"></a>\n",
    "### <b><div style='padding:15px;background-color:#4A5568;color:white;border-radius:2px;font-size:110%;text-align: left'>4. Embeddings & Vector Database (Qdrant)</div></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee5897",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0f7ff,#ffffff);padding:16px;border-left:6px solid #2b6cb0;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>üöÄ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Ce que nous allons faire</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Maintenant que nous avons nos donn√©es enrichies, il est temps de construire notre ¬´ M√©moire Unifi√©e ¬ª.</p>\n",
    "    <p style='margin:0 8px 0 0;font-weight:600;color:#000;'>Base vectorielle (Qdrant)</p>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous allons int√©grer nos fragments et les stocker dans `Qdrant`. La cl√© ici est *ce que* nous int√©grons. Au lieu du simple texte brut, nous allons cr√©er un texte combin√© pour l'int√©gration qui inclut le r√©sum√© et les mots-cl√©s. Cela injecte la compr√©hension du LLM directement dans la repr√©sentation vectorielle.\n",
    "    </p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30c91357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 566 enriched chunks from file.\n",
      "Embedding dimension: 384\n",
      "Creating new collection 'financial_docs'...\n",
      "Qdrant collection 'financial_docs' created and saved to './qdrant_storage'.\n"
     ]
    }
   ],
   "source": [
    "# Load enriched chunks\n",
    "with open('enriched_chunks.json', 'r') as f:\n",
    "    all_enriched_chunks = json.load(f)\n",
    "print(f\"Loaded {len(all_enriched_chunks)} enriched chunks from file.\")\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "embedding_dim = len(list(embedding_model.embed([\"test\"]))[0])\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Configure Qdrant with persistent storage\n",
    "QDRANT_PATH = \"./qdrant_storage\"\n",
    "COLLECTION_NAME = \"financial_docs\"\n",
    "\n",
    "client = qdrant_client.QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "# Recreate collection if it exists\n",
    "try:\n",
    "    client.get_collection(collection_name=COLLECTION_NAME)\n",
    "    print(f\"Qdrant collection '{COLLECTION_NAME}' already exists. Deleting and recreating...\")\n",
    "    client.delete_collection(collection_name=COLLECTION_NAME)\n",
    "except Exception:\n",
    "    print(f\"Creating new collection '{COLLECTION_NAME}'...\")\n",
    "\n",
    "# Create collection with vector configuration\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=qdrant_client.http.models.VectorParams(\n",
    "        size=embedding_dim,\n",
    "        distance=qdrant_client.http.models.Distance.COSINE\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Qdrant collection '{COLLECTION_NAME}' created and saved to '{QDRANT_PATH}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c72c5",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous avons pr√©par√© notre base de donn√©es vectorielle. Nous avons initialis√© un mod√®le d'embedding open source et cr√©√© une collection `Qdrant` configur√©e pour la similarit√© cosinus, qui est adapt√©e √† la recherche s√©mantique sur du texte.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "215daa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 566 texts for embedding.\n",
      "Generating embeddings...\n",
      "Creating points for upsert...\n",
      "Upserting into Qdrant...\n",
      "\n",
      "Upsert complete!\n",
      "Points in collection: 566\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_text(chunk: Dict) -> str:\n",
    "    return f\"\"\"\n",
    "    Summary: {chunk['summary']}\n",
    "    Keywords: {', '.join(chunk['keywords'])}\n",
    "    Content: {chunk['content'][:1000]} \n",
    "    \"\"\"\n",
    "\n",
    "texts_to_embed = [create_embedding_text(chunk) for chunk in all_enriched_chunks]\n",
    "\n",
    "print(f\"Prepared {len(texts_to_embed)} texts for embedding.\")\n",
    "print(\"Generating embeddings...\")\n",
    "\n",
    "embeddings = list(embedding_model.embed(texts_to_embed, batch_size=32))\n",
    "\n",
    "print(\"Creating points for upsert...\")\n",
    "points_to_upsert = []\n",
    "for i, (chunk, embedding) in enumerate(zip(all_enriched_chunks, embeddings)):\n",
    "    points_to_upsert.append(qdrant_client.http.models.PointStruct(\n",
    "        id=i,\n",
    "        vector=embedding.tolist(),\n",
    "        payload=chunk\n",
    "    ))\n",
    "\n",
    "print(\"Upserting into Qdrant...\")\n",
    "client.upsert(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    points=points_to_upsert,\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(\"\\nUpsert complete!\")\n",
    "collection_info = client.get_collection(collection_name=COLLECTION_NAME)\n",
    "print(f\"Points in collection: {collection_info.points_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9bf27",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Notre base de connaissances de fragments enrichis est √† pr√©sent index√©e dans notre base de donn√©es vectorielle. La derni√®re ligne v√©rifie le nombre de points (fragements des documents) dans la collection, qui devrait correspondre au nombre total de fragments enrichis que nous avons cr√©√©s. Notre agent <i>Liberian</i> dispose d√©sormais d'une biblioth√®que enti√®rement peupl√©e dans laquelle effectuer des recherches.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d689ac2",
   "metadata": {},
   "source": [
    "<a id=\"base-donnees-sql\"></a>\n",
    "### <b><div style='padding:15px;background-color:#4A5568;color:white;border-radius:2px;font-size:110%;text-align: left'>5. Cr√©ation de la base de donn√©es SQL</div></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e8475",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0f7ff,#ffffff);padding:16px;border-left:6px solid #2b6cb0;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>üöÄ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Base de donn√©es relationnelle (SQLite)</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous allons charger √† pr√©sent nos donn√©es structur√©es <i>revenue_summary.csv</i> dans une base de donn√©es SQLite pour que notre agent `Analyst` puisse l'interroger.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a97df797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite database created at 'financials.db'.\n",
      "\n",
      "Verifying table schema:\n",
      "\n",
      "CREATE TABLE revenue_summary (\n",
      "\tyear INTEGER, \n",
      "\tquarter TEXT, \n",
      "\trevenue_usd_billions REAL, \n",
      "\tnet_income_usd_billions REAL\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from revenue_summary table:\n",
      "year\tquarter\trevenue_usd_billions\tnet_income_usd_billions\n",
      "2020\tQ1\t3.11\t0.95\n",
      "2020\tQ2\t3.08\t0.92\n",
      "2020\tQ3\t3.87\t0.62\n",
      "*/\n",
      "\n",
      "Verifying sample rows:\n",
      "[(2020, 'Q1', 3.11, 0.95), (2020, 'Q2', 3.08, 0.92), (2020, 'Q3', 3.87, 0.62), (2020, 'Q4', 4.73, 1.34), (2021, 'Q1', 5.0, 1.46)]\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DB_PATH = \"financials.db\"\n",
    "TABLE_NAME = \"revenue_summary\"\n",
    "\n",
    "# Create and populate SQLite database\n",
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    df.to_sql(TABLE_NAME, conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"SQLite database created at '{DB_PATH}'.\")\n",
    "\n",
    "# Verify database and schema\n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{DB_PATH}\")\n",
    "\n",
    "print(\"\\nVerifying table schema:\")\n",
    "print(db.get_table_info())\n",
    "print(\"\\nVerifying sample rows:\")\n",
    "print(db.run(f\"SELECT * FROM {TABLE_NAME} LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93c33e5",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous avons cr√©√© avec la base de donn√©es SQLite et l'avons peupl√©e avec nos donn√©es structur√©es. Le wrapper <code>langchain_community.utilities.SQLDatabase</code> rend incroyablement simple la connexion de cette base de donn√©es √† un agent LLM. Notre agent <i>Analyst</i> est maintenant pr√™t √† l'emploi. Ceci conclut la Phase 1.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76effb54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"phase-terminee\"></a>\n",
    "### Prochaines √©tapes\n",
    "\n",
    "**Ce que nous avons construit :**\n",
    "- Analys√© et d√©coup√© les documents SEC avec pr√©servation de la structure\n",
    "- Enrichi tous les fragments avec des m√©tadonn√©es g√©n√©r√©es par LLM\n",
    "- Cr√©√© une base de donn√©e vectorielle `Qdrant` (sauvegard√© sur disque)\n",
    "- Cr√©√© une base de donn√©es `SQLite` avec des donn√©es financi√®res structur√©es\n",
    "\n",
    "**Sorties cl√©s :**\n",
    "- `enriched_chunks.json` - Tous les fragments de documents enrichis\n",
    "- `financials.db` - Base de donn√©es SQLite\n",
    "- `qdrant_storage/` - Collection Qdrant persistante avec tous les embeddings\n",
    "\n",
    "**Suivant :** [Passez √† la Phase 2](phase_2_specialist_agents.ipynb) pour construire les agents sp√©cialis√©s (nos outils)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388ef09",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
