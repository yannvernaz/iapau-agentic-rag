{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118f20dd",
   "metadata": {},
   "source": [
    "<table style=\"border:none; border-collapse:collapse; cellspacing:0; cellpadding:0\">\n",
    "<tr>\n",
    "    <td width=30% style=\"border:none\">\n",
    "        <center>\n",
    "            <img src=\"../images/iapau_icon.png\" width=\"30%\"/><br>\n",
    "            <a href=\"https://iapau.org/\">Association IA Pau</a><br>\n",
    "            <a href=\"https://iapau.org/events/festival/\">Festival IAPau 7</a>\n",
    "        </center>\n",
    "    </td>\n",
    "    <td style=\"border:none\">\n",
    "        <center>\n",
    "            <h1>Atelier - Agentic RAG</h1>\n",
    "            <h2>Base de Connaissances</h2>\n",
    "            <h2>Ingestion, Enrichissemment, et Indexation</h2>\n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=20% style=\"border:none\">\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "**Pr√©requis :** Compl√©ter d'abord la Phase 0 (acquisition des donn√©es).\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../images/agentic-rag-data-ingestion-iapau.png\" alt=\"data-preparation\" width=\"70%\">\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table des mati√®res\n",
    "\n",
    "1. [**Segmentation des documents**](#segmentation-docs)\n",
    "   - Extraction du contenu des documents\n",
    "   - Segmentation avec la biblioth√®que `unstructured`\n",
    "\n",
    "2. [**D√©coupage s√©mantique/structur√©**](#decoupage-semantique)\n",
    "   - Chunking conscient de la structure\n",
    "   - Pr√©servation des informations hi√©rarchiques\n",
    "\n",
    "3. [**Enrichissement avec LLM**](#enrichissement-llm)\n",
    "   - G√©n√©ration de m√©tadonn√©es structur√©es\n",
    "   - Enrichissement des chunks avec r√©sum√©s et mots-cl√©s\n",
    "\n",
    "4. [**Emdeddings & Vector Database (Qdrant)**](#vector-db)\n",
    "   - G√©n√©ration d'embeddings\n",
    "   - Population de la base vectorielle Qdrant\n",
    "\n",
    "5. [**Cr√©ation de la base de donn√©es SQL**](#bd-sql)\n",
    "   - Cr√©ation de la base SQLite\n",
    "   - Structuration des donn√©es financi√®res\n",
    "\n",
    "[**Prochaine √©tape**](#next)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c3ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from fastembed import TextEmbedding\n",
    "import qdrant_client\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "from IPython.display import display, Markdown, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d29206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Phase 0\n",
    "COMPANY_TICKER = \"NVDA\"\n",
    "DATA_PATH = Path(f\"sec-edgar-filings/{COMPANY_TICKER}/\")\n",
    "CSV_PATH = \"revenue_summary.csv\"\n",
    "\n",
    "# Find all SEC submission files\n",
    "all_files = list(DATA_PATH.rglob(\"full-submission.txt\"))\n",
    "print(f\"Loaded {len(all_files)} files from Phase 0\")\n",
    "\n",
    "# Load the CSV data\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"\\nLoaded structured data with {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a9e30d",
   "metadata": {},
   "source": [
    "<a id=\"segmentation-docs\"></a>\n",
    "### <b><div style='padding:15px;background-color:#4A5568;color:white;border-radius:2px;font-size:110%;text-align: left'>1. Segmentation des documents</div></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e911670f",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0f7ff,#ffffff);padding:16px;border-left:6px solid #2b6cb0;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>üöÄ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Ce que nous allons faire</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous allons utiliser la biblioth√®que <a href=\"https://docs.unstructured.io\">unstructured</a>  pour analyser les d√©p√¥ts HTML bruts.</p> \n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Contrairement √† une simple extraction de texte, <a href=\"https://docs.unstructured.io\">unstructured</a>  partitionne le document en une liste d'¬´ √©l√©ments ¬ª significatifs tels que <code>Title</code>, <code>NarrativeText</code>, <code>ListItem</code>, <code>Table</code> et <code>Figure</code>. Cette pr√©servation de l'information structurelle est la premi√®re et la plus critique √©tape vers un d√©coupage s√©mantique performant.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_html_from_sec_file(file_path) -> str:\n",
    "    \"\"\"Extracts the HTML content from an SEC submission file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    match = re.search(r'<html[^>]*>.*?</html>', content, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return \"\"\n",
    "\n",
    "def parse_html_file(file_path):\n",
    "    \"\"\"Parses an HTML file using unstructured and returns a list of elements.\"\"\"\n",
    "    try:\n",
    "        html_content = extract_html_from_sec_file(file_path)\n",
    "        \n",
    "        if not html_content:\n",
    "            print(\"No HTML content found in file\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Extracted HTML content: {len(html_content):,} characters\")\n",
    "        \n",
    "        from io import BytesIO\n",
    "        # Try with more aggressive detection settings\n",
    "        elements = partition_html(\n",
    "            file=BytesIO(html_content.encode('utf-8')),\n",
    "            # Use hi_res for better element detection (slower but more accurate)\n",
    "            # Note: This requires detectron2 for table detection\n",
    "            # For now, we use default but with include_metadata\n",
    "            include_metadata=True,\n",
    "        )\n",
    "        return elements\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {file_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# Test parsing on first 10-K file\n",
    "ten_k_file = next(f for f in all_files if \"10-K\" in str(f))\n",
    "print(f\"Parsing file: {ten_k_file}...\")\n",
    "\n",
    "parsed_elements = parse_html_file(ten_k_file)\n",
    "\n",
    "print(f\"\\nSuccessfully parsed into {len(parsed_elements)} elements.\")\n",
    "\n",
    "# Show element type distribution\n",
    "from collections import Counter\n",
    "element_types = Counter(elem.category if hasattr(elem, 'category') else type(elem).__name__ \n",
    "                        for elem in parsed_elements)\n",
    "print(\"\\n--- Distribution des types d'√©l√©ments ---\")\n",
    "for elem_type, count in element_types.most_common():\n",
    "    print(f\"{elem_type}: {count}\")\n",
    "\n",
    "# Find and display specific element types\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXEMPLES DE DIFF√âRENTS TYPES D'√âL√âMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find examples of each type\n",
    "title_example = None\n",
    "narrative_example = None\n",
    "table_example = None\n",
    "\n",
    "for element in parsed_elements:\n",
    "    elem_type = element.category if hasattr(element, 'category') else type(element).__name__\n",
    "    \n",
    "    if elem_type == \"Title\" and title_example is None:\n",
    "        title_example = element\n",
    "    elif elem_type == \"NarrativeText\" and narrative_example is None and len(str(element)) > 100:\n",
    "        narrative_example = element\n",
    "    elif elem_type == \"Table\" and table_example is None:\n",
    "        table_example = element\n",
    "    \n",
    "    if title_example and narrative_example and table_example:\n",
    "        break\n",
    "\n",
    "# Display Title example\n",
    "if title_example:\n",
    "    print(\"\\n--- Exemple de TITRE (Title) ---\")\n",
    "    print(f\"Contenu: {str(title_example)}\")\n",
    "else:\n",
    "    print(\"\\n--- Exemple de TITRE (Title) ---\")\n",
    "    print(\"Aucun √©l√©ment Title trouv√© - tous les √©l√©ments sont probablement class√©s comme UncategorizedText\")\n",
    "\n",
    "# Display NarrativeText example\n",
    "if narrative_example:\n",
    "    print(\"\\n--- Exemple de TEXTE NARRATIF (NarrativeText) ---\")\n",
    "    print(f\"Contenu: {str(narrative_example)[:500]}...\")\n",
    "else:\n",
    "    print(\"\\n--- Exemple de TEXTE NARRATIF (NarrativeText) ---\")\n",
    "    print(\"Aucun √©l√©ment NarrativeText trouv√©\")\n",
    "\n",
    "# Display Table example\n",
    "if table_example:\n",
    "    print(\"\\n--- Exemple de TABLEAU (Table) ---\")\n",
    "    # Check if table has HTML metadata\n",
    "    if hasattr(table_example, 'metadata'):\n",
    "        table_metadata = table_example.metadata.to_dict() if hasattr(table_example.metadata, 'to_dict') else {}\n",
    "        if 'text_as_html' in table_metadata:\n",
    "            print(f\"Repr√©sentation HTML:\\n{table_metadata['text_as_html'][:800]}...\")\n",
    "        else:\n",
    "            print(f\"Repr√©sentation texte:\\n{str(table_example)[:500]}...\")\n",
    "    else:\n",
    "        print(f\"Repr√©sentation texte:\\n{str(table_example)[:500]}...\")\n",
    "else:\n",
    "    print(\"\\n--- Exemple de TABLEAU (Table) ---\")\n",
    "    print(\"Aucun √©l√©ment Table trouv√©\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOTE: Si vous ne voyez que 'UncategorizedText', c'est normal pour les fichiers SEC.\")\n",
    "print(\"Le HTML des d√©p√¥ts SEC est tr√®s complexe et unstructured a du mal √† d√©tecter\")\n",
    "print(\"la structure automatiquement. Les chunks fonctionneront quand m√™me correctement.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a27e2",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>La sortie montre que nous avons r√©ussi √† partitionner le document en plusieurs centaines d'√©l√©ments. L'exemple de sortie d√©montre que <a href=\"https://docs.unstructured.io\">unstructured</a>  a identifi√© diff√©rents types de contenu. Nous pouvons voir <code>Table</code> et <code>NarrativeText</code>. Cette premi√®re structuration va nous permettre de cr√©er des fragments s√©mantiques coh√©rents, en particulier pour pr√©server les tableaux.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6fe524",
   "metadata": {},
   "source": [
    "<a id=\"decoupage-semantique\"></a>\n",
    "### <b><div style='padding:15px;background-color:#4A5568;color:white;border-radius:2px;font-size:110%;text-align: left'>2. D√©coupage s√©mantique</div></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de2840",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0f7ff,#ffffff);padding:16px;border-left:6px solid #2b6cb0;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>üöÄ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Ce que nous allons faire</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Les m√©thodes de d√©coupage standard (comme un nombre fixe de charact√®res) peuvent √™tre destructrices, en particulier pour les documents complexes o√π les tableaux sont critiques. Un tableau coup√© en deux perd tout son sens. Nous allons utiliser la strat√©gie <code>chunk_by_title</code>. Cette m√©thode regroupe le texte sous les titres et, surtout, tente de conserver les tableaux entiers, en les traitant comme des unit√©s atomiques.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550702cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_by_title(\n",
    "    parsed_elements,\n",
    "    max_characters=2048,\n",
    "    combine_text_under_n_chars=256,\n",
    "    new_after_n_chars=1800\n",
    ")\n",
    "\n",
    "print(f\"Document chunked into {len(chunks)} sections.\")\n",
    "\n",
    "# Find sample chunks\n",
    "text_chunk_sample = None\n",
    "table_chunk_sample = None\n",
    "\n",
    "for chunk in chunks:\n",
    "    chunk_metadata = chunk.metadata.to_dict() if hasattr(chunk.metadata, 'to_dict') else {}\n",
    "    if 'text_as_html' not in chunk_metadata and text_chunk_sample is None and len(str(chunk)) > 500:\n",
    "        text_chunk_sample = chunk\n",
    "    if 'text_as_html' in chunk_metadata and table_chunk_sample is None:\n",
    "        table_chunk_sample = chunk\n",
    "    if text_chunk_sample and table_chunk_sample:\n",
    "        break\n",
    "\n",
    "if text_chunk_sample:\n",
    "    display(Markdown(f\"\"\"\n",
    "    ---\n",
    "    ### üìÑ Exemple de Chunk de Texte\n",
    "\n",
    "    **Type:** Texte narratif  \n",
    "    **Longueur:** {len(str(text_chunk_sample))} caract√®res\n",
    "\n",
    "    **Contenu:**\n",
    "    ```\n",
    "    {str(text_chunk_sample)[:700]}...\n",
    "    ```\n",
    "    ---\n",
    "    \"\"\"))\n",
    "\n",
    "if table_chunk_sample:\n",
    "    table_metadata = table_chunk_sample.metadata.to_dict() if hasattr(table_chunk_sample.metadata, 'to_dict') else {}\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ---\n",
    "    ### üìä Exemple de Chunk de Tableau\n",
    "\n",
    "    **Type:** Tableau (pr√©serv√© avec HTML)  \n",
    "    **M√©tadonn√©e:** `text_as_html` pr√©sent = ‚úÖ\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Display the actual HTML table if available\n",
    "    if 'text_as_html' in table_metadata:\n",
    "        display(Markdown(\"**Rendu du tableau:**\"))\n",
    "        display(HTML(table_metadata['text_as_html']))\n",
    "        \n",
    "        display(Markdown(f\"\"\"\n",
    "    **Code HTML source (extrait):**\n",
    "    ```html\n",
    "    {table_metadata['text_as_html'][:600]}...\n",
    "    ```\n",
    "    \"\"\"))\n",
    "    else:\n",
    "        display(Markdown(f\"\"\"\n",
    "    **Contenu texte:**\n",
    "    ```\n",
    "    {str(table_chunk_sample)[:500]}...\n",
    "    ```\n",
    "    \"\"\"))\n",
    "    \n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f89ad32",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>La sortie montre que nous avons r√©duit des centaines d'√©l√©ments en quelques fragments. Notez que les m√©tadonn√©es du fragment de tableau incluent <code>text_as_html</code>. Cela indique que <a href=\"https://docs.unstructured.io\">unstructured</a> a correctement identifi√© et pr√©serv√© un tableau, ce qui est un √©norme avantage pour la qualit√© des donn√©es. Nous avons r√©ussi √† √©viter de d√©truire des donn√©es tabulaires critiques pendant le processus de d√©coupage.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8410a",
   "metadata": {},
   "source": [
    "<a id=\"enrichissement-llm\"></a>\n",
    "### <b><div style='padding:15px;background-color:#4A5568;color:white;border-radius:2px;font-size:110%;text-align: left'>3. Enrichissement avec LLM</div></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651475b6",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0f7ff,#ffffff);padding:16px;border-left:6px solid #2b6cb0;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>üöÄ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Ce que nous allons faire</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Au lieu d'int√©grer simplement les fragments de document, nous utilisons un LLM pour g√©n√©rer des m√©tadonn√©es pour chaque fragment. Ces m√©tadonn√©es agissent comme des ¬´ signaux ¬ª suppl√©mentaires pour notre syst√®me RAG, lui permettant de comprendre le contenu √† un niveau beaucoup plus profond.</p>\n",
    "    <p style='margin:0 8px 0 0;font-weight:600;color:#000;'>Pour chaque fragment, nous g√©n√©rerons :</p>\n",
    "    <ul style='margin:8px 0 0 18px;'>\n",
    "      <li><strong>R√©sum√©</strong> : Un r√©sum√© concis de 1 √† 2 phrases.</li>\n",
    "      <li><strong>Mots-cl√©s</strong> : Une liste de sujets cl√©s.</li>\n",
    "      <li><strong>Questions hypoth√©tiques</strong> : Une liste de questions auxquelles le fragment peut r√©pondre.</li>\n",
    "      <li><strong>R√©sum√© de tableau (pour les tableaux uniquement)</strong> : Une description en langage naturel des principales informations du tableau.</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic permet de d√©finir la structure JSON souhait√©e, garantissant une sortie structur√©e du LLM.\n",
    "\n",
    "class ChunkMetadata(BaseModel):\n",
    "    \"\"\"Structured metadata for a document chunk.\"\"\"\n",
    "    summary: str = Field(description=\"A concise 1-2 sentence summary of the chunk.\")\n",
    "    keywords: List[str] = Field(description=\"A list of 5-7 key topics or entities mentioned.\")\n",
    "    hypothetical_questions: List[str] = Field(description=\"A list of 3-5 questions this chunk could answer.\")\n",
    "    table_summary: Optional[str] = Field(description=\"If the chunk is a table, a natural language summary of its key insights.\", default=None)\n",
    "\n",
    "print(\"Pydantic model for metadata defined.\")\n",
    "print(json.dumps(ChunkMetadata.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a28938",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous avons d√©fini le mod√®le Pydantic <code>ChunkMetadata</code>. L'affichage du sch√©ma JSON montre la structure exacte, y compris les noms de champs, les types et les descriptions, que nous demanderons au LLM. Cette utilisation de sorties structur√©es est bien plus fiable que la simple ing√©nierie de prompt.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI LLM for chunk enrichment - GPT-4o-mini\n",
    "enrichment_llm = ChatOpenAI(model='gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=0.).with_structured_output(ChunkMetadata)\n",
    "\n",
    "def generate_enrichment_prompt(chunk_text: str, is_table: bool) -> str:\n",
    "    table_instruction = \"\"\"\n",
    "    This chunk is a TABLE. Your summary should describe the main data points and trends.\n",
    "    \"\"\" if is_table else \"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert financial analyst. Please analyze the following document chunk and generate the specified metadata.\n",
    "    {table_instruction}\n",
    "    Chunk Content:\n",
    "    ---\n",
    "    {chunk_text}\n",
    "    ---\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def enrich_chunk(chunk) -> Dict[str, Any]:\n",
    "    chunk_metadata = chunk.metadata.to_dict() if hasattr(chunk.metadata, 'to_dict') else {}\n",
    "    is_table = 'text_as_html' in chunk_metadata\n",
    "    content = chunk_metadata.get('text_as_html') if is_table else str(chunk)\n",
    "    \n",
    "    truncated_content = content[:3000]\n",
    "    prompt = generate_enrichment_prompt(truncated_content, is_table)\n",
    "    \n",
    "    try:\n",
    "        metadata_obj = enrichment_llm.invoke(prompt)\n",
    "        return metadata_obj.model_dump()\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error enriching chunk: {type(e).__name__}: {str(e)[:200]}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"Enrichment functions and LLM are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2ecd0",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous avons mis en place la logique de base pour l'enrichissement. Nous avons instanci√© un LLM (<i>gpt-4o-mini</i>) et l'avons li√© √† notre mod√®le Pydantic.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test enrichment on text chunk\n",
    "enriched_text_meta = enrich_chunk(text_chunk_sample)\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "---\n",
    "### üîç Enrichissement d'un Chunk de Texte\n",
    "\n",
    "**R√©sultat de l'enrichissement LLM:**\n",
    "\"\"\"))\n",
    "\n",
    "if enriched_text_meta:\n",
    "    display(Markdown(f\"\"\"\n",
    "**üìù R√©sum√©:**\n",
    "> {enriched_text_meta.get('summary', 'N/A')}\n",
    "\n",
    "**üè∑Ô∏è Mots-cl√©s:**\n",
    "{', '.join([f'`{kw}`' for kw in enriched_text_meta.get('keywords', [])])}\n",
    "\n",
    "**‚ùì Questions hypoth√©tiques:**\n",
    "\"\"\"))\n",
    "    for i, q in enumerate(enriched_text_meta.get('hypothetical_questions', []), 1):\n",
    "        display(Markdown(f\"{i}. *{q}*\"))\n",
    "    \n",
    "    display(Markdown(f\"\"\"\n",
    "**üìä JSON complet:**\n",
    "```json\n",
    "{json.dumps(enriched_text_meta, indent=2, ensure_ascii=False)}\n",
    "```\n",
    "---\n",
    "\"\"\"))\n",
    "\n",
    "# Test enrichment on table chunk\n",
    "display(Markdown(\"\"\"\n",
    "### üìä Enrichissement d'un Chunk de Tableau\n",
    "\"\"\"))\n",
    "\n",
    "enriched_table_meta = enrich_chunk(table_chunk_sample)\n",
    "\n",
    "if enriched_table_meta:\n",
    "    display(Markdown(f\"\"\"\n",
    "**üìù R√©sum√©:**\n",
    "> {enriched_table_meta.get('summary', 'N/A')}\n",
    "\n",
    "**üè∑Ô∏è Mots-cl√©s:**\n",
    "{', '.join([f'`{kw}`' for kw in enriched_table_meta.get('keywords', [])])}\n",
    "\n",
    "**‚ùì Questions hypoth√©tiques:**\n",
    "\"\"\"))\n",
    "    for i, q in enumerate(enriched_table_meta.get('hypothetical_questions', []), 1):\n",
    "        display(Markdown(f\"{i}. *{q}*\"))\n",
    "    \n",
    "    if enriched_table_meta.get('table_summary'):\n",
    "        display(Markdown(f\"\"\"\n",
    "**üìã R√©sum√© du tableau (sp√©cifique):**\n",
    "> {enriched_table_meta.get('table_summary')}\n",
    "\"\"\"))\n",
    "    \n",
    "    display(Markdown(f\"\"\"\n",
    "**üìä JSON complet:**\n",
    "```json\n",
    "{json.dumps(enriched_table_meta, indent=2, ensure_ascii=False)}\n",
    "```\n",
    "---\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea9a22",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2><br>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Pour le fragment de texte, nous avons un r√©sum√© clair, des mots-cl√©s pertinents et des questions hypoth√©tiques perspicaces.</p><br>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Pour le fragment de tableau, le LLM l'a correctement identifi√© comme un tableau et a fourni un <i>table_summary</i> qui interpr√®te les donn√©es en langage naturel. D√©sormais, une recherche pourrait correspondre √† ce tableau, m√™me si ces mots exacts ne figurent pas dans le HTML brut (document initial).</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d6109",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENRICHED_CHUNKS_PATH = 'enriched_chunks.json'\n",
    "\n",
    "if os.path.exists(ENRICHED_CHUNKS_PATH):\n",
    "    print(\"Found existing enriched chunks file. Loading from disk.\")\n",
    "    with open(ENRICHED_CHUNKS_PATH, 'r') as f:\n",
    "        all_enriched_chunks = json.load(f)\n",
    "else:\n",
    "    all_enriched_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66fc272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traiter tous les fichiers (cela prend un peu temps - √† ex√©cuter uniquement si n√©cessaire)\n",
    "\n",
    "def process_file(file_path: Path) -> tuple[List[Dict[str, Any]], int, int]:\n",
    "    \"\"\"Process a single SEC filing and return enriched chunks with stats.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (enriched_chunks, success_count, error_count)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_elements = parse_html_file(file_path)\n",
    "        if not parsed_elements:\n",
    "            print(f\"‚ö†Ô∏è  No elements parsed from {file_path.name}\")\n",
    "            return [], 0, 0\n",
    "        \n",
    "        doc_chunks = chunk_by_title(\n",
    "            parsed_elements, \n",
    "            max_characters=2048, \n",
    "            combine_text_under_n_chars=256\n",
    "        )\n",
    "        \n",
    "        print(f\"  üìÑ Created {len(doc_chunks)} chunks, starting enrichment...\")\n",
    "        \n",
    "        enriched_chunks = []\n",
    "        success_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        for idx, chunk in enumerate(tqdm(doc_chunks, desc=\"Enriching chunks\", leave=False), 1):\n",
    "            try:\n",
    "                enrichment_data = enrich_chunk(chunk)\n",
    "                \n",
    "                if enrichment_data:\n",
    "                    chunk_metadata = chunk.metadata.to_dict() if hasattr(chunk.metadata, 'to_dict') else {}\n",
    "                    is_table = 'text_as_html' in chunk_metadata\n",
    "                    content = chunk_metadata.get('text_as_html') if is_table else str(chunk)\n",
    "                    \n",
    "                    enriched_chunks.append({\n",
    "                        'source': f\"{file_path.parent.parent.name}/{file_path.parent.name}\",\n",
    "                        'content': content,\n",
    "                        'is_table': is_table,\n",
    "                        **enrichment_data\n",
    "                    })\n",
    "                    success_count += 1\n",
    "                else:\n",
    "                    error_count += 1\n",
    "                    print(f\"    ‚ö†Ô∏è  Chunk {idx}/{len(doc_chunks)} failed enrichment\")\n",
    "                \n",
    "                # Small delay to avoid overwhelming the LLM endpoint\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                print(f\"    ‚ùå Chunk {idx}/{len(doc_chunks)} raised exception: {type(e).__name__}\")\n",
    "        \n",
    "        return enriched_chunks, success_count, error_count\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical error processing {file_path.name}: {type(e).__name__}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [], 0, 0\n",
    "\n",
    "# Process all files\n",
    "display(Markdown(f\"\"\"\n",
    "### üîÑ Traitement de {len(all_files)} fichiers SEC\n",
    "\n",
    "Ce processus va:\n",
    "1. Parser chaque fichier HTML\n",
    "2. D√©couper en chunks s√©mantiques\n",
    "3. Enrichir chaque chunk avec le LLM\n",
    "4. Sauvegarder progressivement dans `{ENRICHED_CHUNKS_PATH}`\n",
    "\n",
    "**‚è±Ô∏è Temps estim√©:** ~{len(all_files) * 20} minutes\n",
    "\"\"\"))\n",
    "\n",
    "total_success = 0\n",
    "total_errors = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i, file_path in enumerate(tqdm(all_files, desc=\"Processing files\"), 1):\n",
    "    file_start = time.time()\n",
    "    \n",
    "    file_chunks, success, errors = process_file(file_path)\n",
    "    all_enriched_chunks.extend(file_chunks)\n",
    "    total_success += success\n",
    "    total_errors += errors\n",
    "    \n",
    "    # Save progress after each file\n",
    "    with open(ENRICHED_CHUNKS_PATH, 'w') as f:\n",
    "        json.dump(all_enriched_chunks, f, indent=2)\n",
    "    \n",
    "    file_time = time.time() - file_start\n",
    "    elapsed = time.time() - start_time\n",
    "    avg_time = elapsed / i\n",
    "    eta = avg_time * (len(all_files) - i)\n",
    "    \n",
    "    display(Markdown(f\"\"\"‚úÖ **Fichier {i}/{len(all_files)}**: `{file_path.name}`  \n",
    "‚Üí {len(file_chunks)} chunks enrichis ({success} ‚úÖ, {errors} ‚ùå)  \n",
    "‚Üí Total cumul√©: {len(all_enriched_chunks)} chunks  \n",
    "‚Üí Temps: {file_time:.1f}s | ETA: {eta/60:.1f} min\"\"\"))\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "---\n",
    "### ‚ú® Traitement termin√©!\n",
    "\n",
    "**Total:** {len(all_enriched_chunks)} chunks enrichis  \n",
    "**Succ√®s:** {total_success} ‚úÖ  \n",
    "**Erreurs:** {total_errors} ‚ùå  \n",
    "**Taux de r√©ussite:** {100*total_success/(total_success+total_errors):.1f}%  \n",
    "**Temps total:** {(time.time()-start_time)/60:.1f} minutes  \n",
    "**Sauvegard√© dans:** `{ENRICHED_CHUNKS_PATH}`\n",
    "---\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03704d11",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Cette cellule prend un certain temps √† s'ex√©cuter car elle implique de nombreux appels LLM. La sortie contient tous les fragments enrichis en m√©tadonn√©es, que nous avons cr√©√©s √† partir de tous les d√©p√¥ts SEC. Fait crucial, nous avons enregistr√© ce r√©sultat dans un fichier JSON. Il s'agit d'une bonne pratique essentielle pour sauvegarder notre progression et √©viter de r√©ex√©cuter des √©tapes co√ªteuses de traitement des donn√©es.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b1ae5",
   "metadata": {},
   "source": [
    "<a id=\"vector-db\"></a>\n",
    "### <b><div style='padding:15px;background-color:#4A5568;color:white;border-radius:2px;font-size:110%;text-align: left'>4. Embeddings & Vector Database (Qdrant)</div></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee5897",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0f7ff,#ffffff);padding:16px;border-left:6px solid #2b6cb0;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>üöÄ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Ce que nous allons faire</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Maintenant que nous avons nos donn√©es enrichies, il est temps de construire notre ¬´ base de connaissances ¬ª.</p>\n",
    "    <p style='margin:0 8px 0 0;font-weight:600;color:#000;'>Base vectorielle (Qdrant)</p><br>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous allons int√©grer nos fragments et les stocker dans <i>Qdrant</i>. La cl√© ici est <strong>ce que</strong> nous int√©grons. Au lieu du simple texte brut, nous allons cr√©er un texte combin√© pour l'int√©gration qui inclut le r√©sum√© et les mots-cl√©s. Cela injecte les donn√©es enrichies directement dans la repr√©sentation vectorielle.</p><br>\n",
    "    Nous utilison le mod√®le <a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5\">BAAI/bge-small-en-v1.5</a> pour calculer les <i>embeddings<i>.\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c91357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enriched chunks\n",
    "with open('enriched_chunks.json', 'r') as f:\n",
    "    all_enriched_chunks = json.load(f)\n",
    "print(f\"Loaded {len(all_enriched_chunks)} enriched chunks from file.\")\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "embedding_dim = len(list(embedding_model.embed([\"test\"]))[0])\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Configure Qdrant with persistent storage\n",
    "QDRANT_PATH = \"./qdrant_storage\"\n",
    "COLLECTION_NAME = \"financial_docs\"\n",
    "\n",
    "client = qdrant_client.QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "# Recreate collection if it exists\n",
    "try:\n",
    "    client.get_collection(collection_name=COLLECTION_NAME)\n",
    "    print(f\"Qdrant collection '{COLLECTION_NAME}' already exists. Deleting and recreating...\")\n",
    "    client.delete_collection(collection_name=COLLECTION_NAME)\n",
    "except Exception:\n",
    "    print(f\"Creating new collection '{COLLECTION_NAME}'...\")\n",
    "\n",
    "# Create collection with vector configuration\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=qdrant_client.http.models.VectorParams(\n",
    "        size=embedding_dim,\n",
    "        distance=qdrant_client.http.models.Distance.COSINE\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Qdrant collection '{COLLECTION_NAME}' created and saved to '{QDRANT_PATH}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c72c5",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous avons pr√©par√© notre base de donn√©es vectorielle. Nous avons initialis√© un mod√®le d'embedding open source et cr√©√© une collection <i>Qdrant</i> configur√©e pour la similarit√© cosinus.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215daa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_text(chunk: Dict) -> str:\n",
    "    return f\"\"\"\n",
    "    Summary: {chunk['summary']}\n",
    "    Keywords: {', '.join(chunk['keywords'])}\n",
    "    Content: {chunk['content'][:1000]} \n",
    "    \"\"\"\n",
    "\n",
    "texts_to_embed = [create_embedding_text(chunk) for chunk in all_enriched_chunks]\n",
    "\n",
    "print(f\"Prepared {len(texts_to_embed)} texts for embedding.\")\n",
    "print(\"Generating embeddings...\")\n",
    "\n",
    "embeddings = list(embedding_model.embed(texts_to_embed, batch_size=32))\n",
    "\n",
    "print(\"Creating points for upsert...\")\n",
    "points_to_upsert = []\n",
    "for i, (chunk, embedding) in enumerate(zip(all_enriched_chunks, embeddings)):\n",
    "    points_to_upsert.append(qdrant_client.http.models.PointStruct(\n",
    "        id=i,\n",
    "        vector=embedding.tolist(),\n",
    "        payload=chunk\n",
    "    ))\n",
    "\n",
    "print(\"Upserting into Qdrant...\")\n",
    "client.upsert(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    points=points_to_upsert,\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(\"\\nUpsert complete!\")\n",
    "collection_info = client.get_collection(collection_name=COLLECTION_NAME)\n",
    "print(f\"Points in collection: {collection_info.points_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9bf27",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Notre base de connaissances de fragments enrichis est √† pr√©sent index√©e dans notre base de donn√©es vectorielle. La derni√®re ligne v√©rifie le nombre de points (fragements des documents) dans la collection, qui devrait correspondre au nombre total de fragments enrichis que nous avons cr√©√©s. Notre agent <i>Liberian</i> dispose d√©sormais d'une biblioth√®que enti√®rement peupl√©e dans laquelle effectuer des recherches.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d689ac2",
   "metadata": {},
   "source": [
    "<a id=\"bb-sql\"></a>\n",
    "### <b><div style='padding:15px;background-color:#4A5568;color:white;border-radius:2px;font-size:110%;text-align: left'>5. Cr√©ation de la base de donn√©es SQL</div></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e8475",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0f7ff,#ffffff);padding:16px;border-left:6px solid #2b6cb0;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>üöÄ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Base de donn√©es relationnelle (SQLite)</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous allons charger √† pr√©sent nos donn√©es structur√©es <i>revenue_summary.csv</i> dans une base de donn√©es SQLite pour que notre agent <i>Analyst</i> puisse l'interroger.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97df797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DB_PATH = \"financials.db\"\n",
    "TABLE_NAME = \"revenue_summary\"\n",
    "\n",
    "# Create and populate SQLite database\n",
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    df.to_sql(TABLE_NAME, conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"SQLite database created at '{DB_PATH}'.\")\n",
    "\n",
    "# Verify database and schema\n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{DB_PATH}\")\n",
    "\n",
    "print(\"\\nVerifying table schema:\")\n",
    "print(db.get_table_info())\n",
    "print(\"\\nVerifying sample rows:\")\n",
    "print(db.run(f\"SELECT * FROM {TABLE_NAME} LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93c33e5",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion de la sortie</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous avons cr√©√© avec la base de donn√©es SQLite et l'avons peupl√©e avec nos donn√©es structur√©es. Le wrapper <code>langchain_community.utilities.SQLDatabase</code> rend incroyablement simple la connexion de cette base de donn√©es √† un agent LLM. Notre agent <i>Analyst</i> est maintenant pr√™t √† l'emploi. Ceci conclut la Phase 1.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76effb54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"next\"></a>\n",
    "### Prochaine √©tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ba8e62",
   "metadata": {},
   "source": [
    "**Ce que nous avons construit :**\n",
    "- Analys√© et d√©coup√© les documents SEC avec pr√©servation de la structure\n",
    "- Enrichi tous les fragments avec des m√©tadonn√©es g√©n√©r√©es par LLM\n",
    "- Cr√©√© une base de donn√©e vectorielle `Qdrant` (sauvegard√© sur disque)\n",
    "- Cr√©√© une base de donn√©es `SQLite` avec des donn√©es financi√®res structur√©es\n",
    "\n",
    "**Sorties cl√©s :**\n",
    "- `enriched_chunks.json` - Tous les fragments de documents enrichis\n",
    "- `financials.db` - Base de donn√©es SQLite\n",
    "- `qdrant_storage/` - Collection Qdrant persistante avec tous les embeddings\n",
    "\n",
    "**Suivant :** [Passez √† la Phase 2](phase_2_specialist_agents.ipynb) pour construire les agents sp√©cialis√©s (nos outils)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388ef09",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
