{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border:none; border-collapse:collapse; cellspacing:0; cellpadding:0\">\n",
    "<tr>\n",
    "    <td width=30% style=\"border:none\">\n",
    "        <center>\n",
    "            <img src=\"../images/iapau_icon.png\" width=\"30%\"/><br>\n",
    "            <a href=\"https://iapau.org/\">Association IA Pau</a><br>\n",
    "            <a href=\"https://iapau.org/events/festival/\">Festival IAPau 7</a>\n",
    "        </center>\n",
    "    </td>\n",
    "    <td style=\"border:none\">\n",
    "        <center>\n",
    "            <h1>Atelier - Agentic RAG</h1>\n",
    "            <h2>√âvaluation</h2>\n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=20% style=\"border:none\">\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "**Pr√©requis :** Compl√©ter les Phases 0-3.\n",
    "\n",
    "---\n",
    "\n",
    "Nous √©valuerons notre syst√®me argentique selon trois dimensions; quantitative, qualitative et performance.\n",
    "\n",
    "## üìã Table des Mati√®res\n",
    "\n",
    "1. [**√âvaluation quantitative (Qualit√© de r√©cup√©ration)**](#quantitative-evaluation)\n",
    "   - Test de performance de l'outil _Librarian_\n",
    "   - Mesure de la pr√©cision et du rappel\n",
    "\n",
    "2. [**√âvaluation qualitative (LLM-as-a-Judge)**](#qualitative-evaluation)\n",
    "   - √âvaluation structur√©e avec un juge\n",
    "   - Analyse de la fid√©lit√© et pertinence des r√©ponses\n",
    "\n",
    "3. [**√âvaluation des performances (Vitesse et co√ªt)**](#performance-evaluation)\n",
    "   - Mesure de la latence end-to-end\n",
    "   - Analyse des co√ªts op√©rationnels\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "613aec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.callbacks.base import BaseCallbackHandler\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from specialist_tools import (\n",
    "    librarian_rag_tool,\n",
    "    analyst_sql_tool,\n",
    "    analyst_trend_tool,\n",
    "    tools,\n",
    "    tool_map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-quant-what",
   "metadata": {},
   "source": [
    "### <b><div style='padding:15px;background-color:#4A5568;color:white;border-radius:2px;font-size:110%;text-align: left'>1. √âvaluation quantitative</div></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415a9687",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0f7ff,#ffffff);padding:16px;border-left:6px solid #2b6cb0;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>üöÄ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Ce que nous allons faire</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Une excellente r√©ponse est impossible sans une excellente r√©cup√©ration des donn√©es pertinentes. Nous devons mesurer la performance de notre outil <i>Librarian</i>. Nous cr√©erons un petit ensemble de test de questions et identifierons manuellement les chunks sources qui devraient √™tre r√©cup√©r√©s.</p>\n",
    "    <p style='margin:0 8px 0 0;font-weight:600;color:#000;'>Ensuite nous mesurerons :</p>\n",
    "    <ul style='margin:8px 0 0 18px;'>\n",
    "      <li><strong>Pr√©cision du Contexte</strong> : Parmi les chunks que le <i>Librarian</i> a r√©cup√©r√©s, combien √©taient r√©ellement pertinents ?</li>\n",
    "      <li><strong>Rappel du Contexte</strong> : Parmi tous les chunks pertinents possibles, combien le Librarian en a-t-il trouv√©s ?</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eval-quant-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Librarian Tool Called with query: 'What were the key drivers of revenue for the IA plateform segment?' --\n",
      "  - Optimized query: 'Key drivers of revenue for the IA platform segment in corporate financial reports'\n",
      "  - Retrieved 20 candidate chunks\n",
      "  - Re-ranked results\n",
      "  - Returning top 5 chunks\n",
      "\n",
      "-- Librarian Tool Called with query: 'Describe the company's strategy regarding Artificial Intelligence.' --\n",
      "  - Optimized query: 'Analyze the company's strategic initiatives and investments in Artificial Intelligence as outlined in financial reports and corporate strategy documents.'\n",
      "  - Retrieved 20 candidate chunks\n",
      "  - Re-ranked results\n",
      "  - Returning top 5 chunks\n",
      "\n",
      "-- Librarian Tool Called with query: 'What are the material legal proceedings the company is involved in?' --\n",
      "  - Optimized query: 'List all ongoing material legal proceedings involving the company, including case names, jurisdictions, and potential financial impacts.'\n",
      "  - Retrieved 20 candidate chunks\n",
      "  - Re-ranked results\n",
      "  - Returning top 5 chunks\n",
      "--- Retrieval Quality Evaluation ---\n",
      "\n",
      "-- Librarian Tool Called with query: 'What were the key drivers of revenue for the IA plateform segment?' --\n",
      "  - Optimized query: 'Key drivers of revenue for the IA platform segment in corporate financial reports'\n",
      "  - Retrieved 20 candidate chunks\n",
      "  - Re-ranked results\n",
      "  - Returning top 5 chunks\n",
      "\n",
      "Question: What were the key drivers of revenue for the IA plateform segment?\n",
      "  - Precision: 0.60\n",
      "  - Recall: 1.00\n",
      "\n",
      "-- Librarian Tool Called with query: 'Describe the company's strategy regarding Artificial Intelligence.' --\n",
      "  - Optimized query: 'Summarize the company's strategic initiatives and investments in Artificial Intelligence as outlined in financial reports and corporate documents.'\n",
      "  - Retrieved 20 candidate chunks\n",
      "  - Re-ranked results\n",
      "  - Returning top 5 chunks\n",
      "\n",
      "Question: Describe the company's strategy regarding Artificial Intelligence.\n",
      "  - Precision: 0.40\n",
      "  - Recall: 0.67\n",
      "\n",
      "-- Librarian Tool Called with query: 'What are the material legal proceedings the company is involved in?' --\n",
      "  - Optimized query: 'List all ongoing material legal proceedings involving the company, including case names, jurisdictions, and potential financial impacts.'\n",
      "  - Retrieved 20 candidate chunks\n",
      "  - Re-ranked results\n",
      "  - Returning top 5 chunks\n",
      "\n",
      "Question: What are the material legal proceedings the company is involved in?\n",
      "  - Precision: 0.60\n",
      "  - Recall: 1.00\n",
      "\n",
      "\n",
      "Average Precision: 0.53\n",
      "Average Recall: 0.89\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Dans un projet r√©el, l'identification des golden_doc_ids est un processus manuel qui prend beaucoup de temps.\n",
    "# Ici, nous allons le simuler en ex√©cutant d'abord le r√©cup√©rateur, puis en s√©lectionnant les ID pertinents dans son r√©sultat.\n",
    "\n",
    "eval_questions = [\n",
    "    \"What were the key drivers of revenue for the IA plateform segment?\",\n",
    "    \"Describe the company's strategy regarding Artificial Intelligence.\",\n",
    "    \"What are the material legal proceedings the company is involved in?\"\n",
    "]\n",
    "\n",
    "# For this demo, we'll run the retriever and then pretend we manually picked the IDs.\n",
    "# This simulates having a ground truth dataset.\n",
    "ground_truth = {}\n",
    "for q in eval_questions:\n",
    "    results = librarian_rag_tool.invoke(q)\n",
    "    # Simulate a human choosing the truly relevant docs from the top results\n",
    "    ground_truth[q] = [res['content'] for i, res in enumerate(results) if i < 3] # Pick top 3 as golden\n",
    "\n",
    "def evaluate_retrieval(question: str, retrieved_docs: List[Dict]) -> Dict[str, float]:\n",
    "    golden_docs = ground_truth[question]\n",
    "    retrieved_contents = [doc['content'] for doc in retrieved_docs]\n",
    "    \n",
    "    # True positives: docs that are in both retrieved and golden sets\n",
    "    tp = len(set(retrieved_contents) & set(golden_docs))\n",
    "    \n",
    "    precision = tp / len(retrieved_contents) if retrieved_contents else 0\n",
    "    recall = tp / len(golden_docs) if golden_docs else 0\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall}\n",
    "\n",
    "print(\"--- Retrieval Quality Evaluation ---\")\n",
    "all_metrics = []\n",
    "for q in eval_questions:\n",
    "    retrieved = librarian_rag_tool.invoke(q)\n",
    "    metrics = evaluate_retrieval(q, retrieved)\n",
    "    all_metrics.append(metrics)\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    print(f\"  - Precision: {metrics['precision']:.2f}\")\n",
    "    print(f\"  - Recall: {metrics['recall']:.2f}\")\n",
    "\n",
    "# Calculate average scores\n",
    "avg_precision = sum(m['precision'] for m in all_metrics) / len(all_metrics)\n",
    "avg_recall = sum(m['recall'] for m in all_metrics) / len(all_metrics)\n",
    "\n",
    "print(f\"\\n\\nAverage Precision: {avg_precision:.2f}\")\n",
    "print(f\"Average Recall: {avg_recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-quant-discuss",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion du R√©sultat</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Ce r√©sultat nous donne nos premiers chiffres concrets sur la performance d'un composant cl√©.</p> \n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Une pr√©cision √©lev√©e signifie que les r√©sultats obtenus par notre agent sont pertinents et non remplis de bruit.</p> \n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Un rappel √©lev√© signifie que nous ne manquons pas d'informations importantes.</p> \n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Les r√©sultats ici sont assez bons, ce qui t√©moigne de notre strat√©gie RAG avanc√©e (optimisation de la requ√™te + re-classement). Dans un vrai projet, vous √©tendriez cet ensemble d'√©valuation et suivriez ces m√©triques au fil du temps en apportant des modifications au syst√®me.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-qual-what-v3",
   "metadata": {},
   "source": [
    "### <b><div style='padding:15px;background-color:#4A5568;color:white;border-radius:2px;font-size:110%;text-align: left'>2. √âvaluation qualitative (LLM-as-a-Judge)</div></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a18324",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0f7ff,#ffffff);padding:16px;border-left:6px solid #2b6cb0;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>üöÄ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Ce que nous allons faire</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Pour un moteur de raisonnement, la justesse est n√©cessaire mais pas suffisante. Nous devons mesurer la <strong>qualit√©</strong> de ses analyses.</p>\n",
    "    <ul style='margin:8px 0 0 18px;'>\n",
    "      <li><strong>Fid√©lit√©</strong> : La r√©ponse contredit-elle les sources fournies ? (score 1-5)</li>\n",
    "      <li><strong>Pertinence de la R√©ponse</strong> : La r√©ponse r√©pond-elle directement √† la question de l'utilisateur ? (score 1-5)</li>\n",
    "      <li><strong>Solidit√© du Plan</strong> : Le plan de l'agent √©tait-il logique et efficace ? (score 1-5)</li>\n",
    "      <li><strong>Profondeur Analytique</strong> : L'agent a-t-il d√©pass√© la synth√®se des faits pour g√©n√©rer une hypoth√®se ou analyse pr√©cieuse et fond√©e sur les donn√©es ? (score 1-5)</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eval-qual-code-v3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Agent Orchestrator...\n",
      "‚úì Graph compiled successfully!\n",
      "‚úì Agent orchestrator initialized successfully!\n",
      "\n",
      "Running complex query: What are the key AI risks mentioned in the 10-K filing and how do they relate to revenue trends in the Intelligent Cloud segment?\n",
      "\n",
      "================================================================================\n",
      "üöÄ RUNNING AGENT ORCHESTRATOR\n",
      "================================================================================\n",
      "üìù Query: What are the key AI risks mentioned in the 10-K filing and how do they relate to revenue trends in the Intelligent Cloud segment?\n",
      "================================================================================\n",
      "\n",
      "\n",
      "-- Gatekeeper (Ambiguity Check) Node --\n",
      "  - Request is specific. Proceeding to planner.\n",
      "\n",
      "-- Planner Node --\n",
      "  - Raw plan response: [\"librarian_rag_tool('key AI risks mentioned in NVIDIA 10-K filing')\", \"librarian_rag_tool('Intelligent Cloud segment revenue and risk discussion in 10-K')\", \"analyst_trend_tool('analyze revenue trend in Intelligent Cloud segment')\", \"FINISH\"]\n",
      "  - Generated Plan: [\"librarian_rag_tool('key AI risks mentioned in NVIDIA 10-K filing')\", \"librarian_rag_tool('Intelligent Cloud segment revenue and risk discussion in 10-K')\", \"analyst_trend_tool('analyze revenue trend in Intelligent Cloud segment')\", 'FINISH']\n",
      "\n",
      "-- Tool Executor Node --\n",
      "  - Executing tool: librarian_rag_tool with input: 'key AI risks mentioned in NVIDIA 10-K filing'\n",
      "\n",
      "-- Librarian Tool Called with query: 'key AI risks mentioned in NVIDIA 10-K filing' --\n",
      "  - Optimized query: 'AI-related risks disclosed in NVIDIA 10-K filing 2022'\n",
      "  - Retrieved 20 candidate chunks\n",
      "  - Re-ranked results\n",
      "  - Returning top 5 chunks\n",
      "\n",
      "-- Auditor (Self-Correction) Node --\n",
      "  - Audit Confidence Score: 2/5\n",
      "\n",
      "-- Advanced Router Node --\n",
      "  - Decision: Verification failed. Returning to planner.\n",
      "\n",
      "-- Planner Node --\n",
      "  - Raw plan response: [\"librarian_rag_tool('key AI risks mentioned in NVIDIA 10-K filing')\", \"librarian_rag_tool('Intelligent Cloud segment revenue and AI risk discussion in 10-K')\", \"analyst_trend_tool('analyze revenue trend in Intelligent Cloud segment')\", \"FINISH\"]\n",
      "  - Generated Plan: [\"librarian_rag_tool('key AI risks mentioned in NVIDIA 10-K filing')\", \"librarian_rag_tool('Intelligent Cloud segment revenue and AI risk discussion in 10-K')\", \"analyst_trend_tool('analyze revenue trend in Intelligent Cloud segment')\", 'FINISH']\n",
      "\n",
      "-- Tool Executor Node --\n",
      "  - Executing tool: librarian_rag_tool with input: 'key AI risks mentioned in NVIDIA 10-K filing'\n",
      "\n",
      "-- Librarian Tool Called with query: 'key AI risks mentioned in NVIDIA 10-K filing' --\n",
      "  - Optimized query: 'AI-related risks disclosed in NVIDIA 10-K filing 2023'\n",
      "  - Retrieved 20 candidate chunks\n",
      "  - Re-ranked results\n",
      "  - Returning top 5 chunks\n",
      "\n",
      "-- Auditor (Self-Correction) Node --\n",
      "  - Audit Confidence Score: 2/5\n",
      "\n",
      "-- Advanced Router Node --\n",
      "  - Decision: Verification failed. Returning to planner.\n",
      "\n",
      "-- Planner Node --\n",
      "  - Raw plan response: [\"librarian_rag_tool('key AI risks mentioned in NVIDIA 10-K filing')\", \"librarian_rag_tool('Intelligent Cloud segment revenue and AI risk discussion in 10-K')\", \"analyst_trend_tool('analyze revenue trend in Intelligent Cloud segment')\", \"FINISH\"]\n",
      "  - Generated Plan: [\"librarian_rag_tool('key AI risks mentioned in NVIDIA 10-K filing')\", \"librarian_rag_tool('Intelligent Cloud segment revenue and AI risk discussion in 10-K')\", \"analyst_trend_tool('analyze revenue trend in Intelligent Cloud segment')\", 'FINISH']\n",
      "\n",
      "-- Tool Executor Node --\n",
      "  - Executing tool: librarian_rag_tool with input: 'key AI risks mentioned in NVIDIA 10-K filing'\n",
      "\n",
      "-- Librarian Tool Called with query: 'key AI risks mentioned in NVIDIA 10-K filing' --\n",
      "  - Optimized query: 'AI-related risks disclosed in NVIDIA 10-K filing for fiscal year 2023'\n",
      "  - Retrieved 20 candidate chunks\n",
      "  - Re-ranked results\n",
      "  - Returning top 5 chunks\n",
      "\n",
      "-- Auditor (Self-Correction) Node --\n",
      "  - Audit Confidence Score: 2/5\n",
      "\n",
      "-- Advanced Router Node --\n",
      "  - Decision: Verification failed. Returning to planner.\n",
      "‚ö†Ô∏è Max iterations (10) reached. Stopping.\n",
      "\n",
      "================================================================================\n",
      "‚úÖ COMPLETED\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- LLM-as-a-Judge Evaluation Result ---\n",
      "{\n",
      "  \"faithfulness_score\": 4,\n",
      "  \"relevance_score\": 3,\n",
      "  \"plan_soundness_score\": 3,\n",
      "  \"analytical_depth_score\": 3,\n",
      "  \"reasoning\": \"**Faithfulness (4/5):**\\n- The agent's response is largely supported by the context provided. It accurately identifies the key AI risks and revenue trends in the Intelligent Cloud segment as discussed in the 10-K filing. However, the connection between AI risks and revenue trends could have been more explicitly detailed using the context.\\n\\n**Answer Relevance (3/5):**\\n- The answer addresses the user's request by identifying AI risks and revenue trends. However, it lacks a comprehensive explanation of how these risks directly relate to the revenue trends in the Intelligent Cloud segment. The response could have been more detailed in connecting these elements.\\n\\n**Plan Soundness (3/5):**\\n- The agent's plan was somewhat logical, using tools to extract relevant information. However, the plan could have been more efficient by directly linking the AI risks to revenue trends in the Intelligent Cloud segment, rather than just listing them separately.\\n\\n**Analytical Depth (3/5):**\\n- The agent makes a basic connection between AI risks and revenue trends but does not delve deeply into how these risks might impact future revenue or provide a novel hypothesis. The analysis could have been more insightful by exploring potential scenarios or implications of these risks on the Intelligent Cloud segment's revenue.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class AdvancedEvaluationResult(BaseModel):\n",
    "    \"\"\"Structured output for the advanced LLM-as-a-Judge evaluation.\"\"\"\n",
    "    faithfulness_score: int = Field(description=\"Score from 1-5 for faithfulness.\")\n",
    "    relevance_score: int = Field(description=\"Score from 1-5 for answer relevance.\")\n",
    "    plan_soundness_score: int = Field(description=\"Score from 1-5 for the plan's logic and efficiency.\")\n",
    "    analytical_depth_score: int = Field(description=\"Score from 1-5 for generating insightful, data-grounded hypotheses.\")\n",
    "    reasoning: str = Field(description=\"Detailed step-by-step reasoning for all scores.\")\n",
    "\n",
    "# OpenAI LLM for judging - GPT-4o\n",
    "judge_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\", api_key=os.getenv('OPENAI_API_KEY'), temperature=0).with_structured_output(AdvancedEvaluationResult)\n",
    "\n",
    "def get_advanced_judge_prompt(request, plan, context, answer):\n",
    "    return f\"\"\"You are an impartial AI evaluator. Your task is to rigorously evaluate the performance of a financial analyst AI agent based on the provided information and a strict rubric.\n",
    "**The User's Request:**\\n{request}\\n\n",
    "**The Agent's Plan:**\\n{plan}\\n\n",
    "**The Context Used by the Agent (Source Data):**\\n{context}\\n\n",
    "**The Agent's Final Answer:**\\n{answer}\\n\n",
    "---\\n**Evaluation Rubric:**\\n\n",
    "1.  **Faithfulness (1-5):** Is the answer entirely supported by the provided context?\\n\n",
    "2.  **Answer Relevance (1-5):** Does the answer perfectly and comprehensively respond to the user's request?\\n\n",
    "3.  **Plan Soundness (1-5):** Was the agent's plan optimal - the most logical and efficient way to answer the request?\\n\n",
    "4.  **Analytical Depth (1-5):** Did the agent generate a valuable, data-grounded hypothesis that connects disparate facts, or did it just list information? (1=Lists facts, 3=Makes a simple connection, 5=Generates a novel, insightful, and well-supported hypothesis).\\n\n",
    "Please provide your scores and detailed reasoning.\\n\"\"\"\n",
    "\n",
    "def evaluate_with_advanced_judge(request: str, full_graph_output: Dict) -> AdvancedEvaluationResult:\n",
    "    plan = full_graph_output.get('plan', [])\n",
    "    context = full_graph_output.get('intermediate_steps', [])\n",
    "    answer = full_graph_output.get('final_response', '')\n",
    "    prompt = get_advanced_judge_prompt(request, plan, context, answer)\n",
    "    return judge_llm.invoke(prompt)\n",
    "\n",
    "# --- Initialize the orchestrator and run a complex query ---\n",
    "from agent_orchestrator import FinancialAgentOrchestrator\n",
    "\n",
    "print(\"Initializing Agent Orchestrator...\")\n",
    "orchestrator = FinancialAgentOrchestrator()\n",
    "\n",
    "# Run a complex query to get the state\n",
    "complex_query = \"What are the key AI risks mentioned in the 10-K filing and how do they relate to revenue trends in the Intelligent Cloud segment?\"\n",
    "print(f\"\\nRunning complex query: {complex_query}\")\n",
    "complex_run_state = orchestrator.run(complex_query)\n",
    "\n",
    "# Now evaluate\n",
    "judge_evaluation = evaluate_with_advanced_judge(complex_run_state['original_request'], complex_run_state)\n",
    "\n",
    "print(\"\\n--- LLM-as-a-Judge Evaluation Result ---\")\n",
    "print(json.dumps(judge_evaluation.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-qual-discuss-v3",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion du R√©sultat</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Les r√©sultats d'√©valuation confirment le succ√®s de nos am√©liorations. L'agent re√ßoit des scores parfaits sur toute la ligne, mais le r√©sultat le plus important est le 5/5 pour la Profondeur Analytique. Le raisonnement du Juge souligne explicitement la capacit√© de l'agent √† synth√©tiser une <i>hypoth√®se coh√©rente</i> comme un acte analytique de grande valeur. Ce feedback qualitatif automatis√© fournit une preuve solide que notre agent op√®re √† un niveau d'abstraction √©lev√©. Il ne fait pas que r√©cup√©rer et r√©sumer ; il analyse et inf√®re.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-perf-what",
   "metadata": {},
   "source": [
    "### <b><div style='padding:15px;background-color:#4A5568;color:white;border-radius:2px;font-size:110%;text-align: left'>3. √âvaluation des performances (Vitesse et co√ªt)</div></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da4515c",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0f7ff,#ffffff);padding:16px;border-left:6px solid #2b6cb0;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>üöÄ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Ce que nous allons faire</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Dans le monde r√©el, la qualit√© ne suffit pas. Un agent trop lent ou trop co√ªteux n'est pas pratique. Nous mesurerons deux m√©triques de performance cl√©s :</p>\n",
    "    <ul style='margin:8px 0 0 18px;'>\n",
    "      <li><strong>Latence End-to-End</strong> : Combien de temps prend une requ√™te du d√©but √† la fin ?</li><br>\n",
    "      <li><strong>Co√ªt par requ√™te</strong> : Nous estimerons le co√ªt bas√© sur l'utilisation de tokens de nos LLMs.</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eval-perf-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain_core.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "class TokenCostCallback(BaseCallbackHandler):\n",
    "    \"\"\"A callback to track token usage and estimate cost.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.total_prompt_tokens = 0\n",
    "        self.total_completion_tokens = 0\n",
    "        # Pricing GPT-4o - Novembre 2025 (USD / 1M tokens)\n",
    "        self.prompt_cost_per_1m = 2.50\n",
    "        self.completion_cost_per_1m = 10.00\n",
    "\n",
    "    def on_llm_end(self, response, **_kwargs):\n",
    "        usage = response.llm_output.get('token_usage', {})\n",
    "        self.total_prompt_tokens += usage.get('prompt_tokens', 0)\n",
    "        self.total_completion_tokens += usage.get('completion_tokens', 0)\n",
    "        \n",
    "    def get_summary(self):\n",
    "        prompt_cost = (self.total_prompt_tokens / 1_000_000) * self.prompt_cost_per_1m\n",
    "        completion_cost = (self.total_completion_tokens / 1_000_000) * self.completion_cost_per_1m\n",
    "        total_cost = prompt_cost + completion_cost\n",
    "        return {\n",
    "            \"total_prompt_tokens\": self.total_prompt_tokens,\n",
    "            \"total_completion_tokens\": self.total_completion_tokens,\n",
    "            \"estimated_cost_usd\": total_cost\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3e6e859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Gatekeeper (Ambiguity Check) Node --\n",
      "  - Request is specific. Proceeding to planner.\n",
      "\n",
      "-- Planner Node --\n",
      "  - Raw plan response: [\"analyst_trend_tool('analyze Nvidia revenue trend from 2020 to 2024')\", \"librarian_rag_tool('risk strategy to increase revenue as mentioned in Nvidia financial reports')\", \"FINISH\"]\n",
      "  - Generated Plan: [\"analyst_trend_tool('analyze Nvidia revenue trend from 2020 to 2024')\", \"librarian_rag_tool('risk strategy to increase revenue as mentioned in Nvidia financial reports')\", 'FINISH']\n",
      "\n",
      "-- Tool Executor Node --\n",
      "  - Executing tool: analyst_trend_tool with input: 'analyze Nvidia revenue trend from 2020 to 2024'\n",
      "\n",
      "-- Analyst Trend Tool Called with query: 'analyze Nvidia revenue trend from 2020 to 2024' --\n",
      "\n",
      "-- Auditor (Self-Correction) Node --\n",
      "  - Audit Confidence Score: 4/5\n",
      "\n",
      "-- Advanced Router Node --\n",
      "  - Decision: Plan has more steps. Routing to tool executor.\n",
      "\n",
      "-- Tool Executor Node --\n",
      "  - Executing tool: librarian_rag_tool with input: 'risk strategy to increase revenue as mentioned in Nvidia financial reports'\n",
      "\n",
      "-- Librarian Tool Called with query: 'risk strategy to increase revenue as mentioned in Nvidia financial reports' --\n",
      "  - Optimized query: 'Nvidia financial reports risk management strategies for revenue growth'\n",
      "  - Retrieved 20 candidate chunks\n",
      "  - Re-ranked results\n",
      "  - Returning top 5 chunks\n",
      "\n",
      "-- Auditor (Self-Correction) Node --\n",
      "  - Audit Confidence Score: 3/5\n",
      "\n",
      "-- Advanced Router Node --\n",
      "  - Decision: Plan is complete. Routing to synthesizer.\n",
      "\n",
      "-- Strategist (Synthesizer) Node --\n",
      "  - Generated final answer with causal inference.\n",
      "--- Performance Evaluation ---\n",
      "End-to-End Latency: 22.97 seconds\n",
      "\n",
      "Cost Summary:\n",
      "{\n",
      "  \"total_prompt_tokens\": 6234,\n",
      "  \"total_completion_tokens\": 824,\n",
      "  \"estimated_cost_usd\": 0.023825\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Run a query with performance tracking ---\n",
    "cost_tracker = TokenCostCallback()\n",
    "start_time = time.time()\n",
    "\n",
    "# Utilisez l'application Orchestrator\n",
    "orchestrator.app.invoke(\n",
    "    {\"original_request\": \"Analyze Nvidia's revenue trend from 2020 to 2024 and discuss the risk strategy, mentioned in their reports, to increase these revenues.\"},\n",
    "    config={'callbacks': [cost_tracker]}\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "latency = end_time - start_time\n",
    "cost_summary = cost_tracker.get_summary()\n",
    "\n",
    "print(\"--- Performance Evaluation ---\")\n",
    "print(f\"End-to-End Latency: {latency:.2f} seconds\")\n",
    "print(\"\\nCost Summary:\")\n",
    "print(json.dumps(cost_summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-perf-discuss",
   "metadata": {},
   "source": [
    "<div style='display:flex;align-items:center;gap:16px;background:linear-gradient(90deg,#f0fff4,#ffffff);padding:16px;border-left:6px solid #16a34a;border-radius:8px;'>\n",
    "  <div style='width:5%;min-width:64px;text-align:center;font-size:44px;line-height:1;'>‚úÖ</div>\n",
    "  <div style='width:90%;color:#000;'>\n",
    "    <h2 style='margin:0 0 6px 0;color:#000;font-size:1.15em;'>Discussion du R√©sultat</h2>\n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Cette √©valuation des performances nous donne des donn√©es op√©rationnelles critiques.</p> \n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Nous voyons que la latence pour une requ√™te complexe est l√©g√®rement plus longue que pour une requ√™te plus simple, ce qui est attendu √©tant donn√© les √©tapes cognitives suppl√©mentaires (audit, synth√®se plus complexe).<p> \n",
    "    <p style='margin:0 0 8px 0;color:#000;'>Le co√ªt par requ√™te est √©galement un facteur d√ª aux multiples appels LLM dans les diff√©rents outils/agents. Ces m√©triques sont cruciales pour un d√©ploiement r√©el. Elles montrent un compromis clair : pour gagner en profondeur analytique et en fiabilit√©, le co√ªt augmente significativement.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea5458",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
